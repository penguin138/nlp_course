{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging with maximum entropy models (10 pts)\n",
    "\n",
    "In this task you will build a maximum entropy model for part-of-speech tagging. As the name suggests, our problem is all about converting a sequence of words into a sequence of part-of-speech tags. \n",
    "<img src=https://i.stack.imgur.com/6pdIT.png width=320>\n",
    "\n",
    "\n",
    "__Your man goal:__ implement the model from [the article you're given](W96-0213.pdf).\n",
    "\n",
    "Unlike previous tasks, this one gives you greater degree of freedom and less automated tests. We provide you with programming interface but nothing more.\n",
    "\n",
    "__A piece of advice:__ there's a lot of objects happening here. If you don't understand why some object is needed, find `def train` function and see how everything is linked together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import os\n",
    "import pprint\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I: reading input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types:\n",
    "# Word: str\n",
    "# Sentence: list of str\n",
    "TaggedWord = collections.namedtuple('TaggedWord', ['text', 'tag'])\n",
    "# TaggedSentence: list of TaggedWord\n",
    "# Tags: list of TaggedWord\n",
    "# TagLattice: list of Tags\n",
    "\n",
    "def read_tagged_sentences(path):\n",
    "    \"\"\"\n",
    "    Read tagged sentences from CoNLL-U file and return array of TaggedSentence (array of lists of TaggedWord).\n",
    "    \"\"\"\n",
    "    tagged_sentences = []\n",
    "    with open(path) as fin:\n",
    "        sentence = []\n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            if not line:\n",
    "                if sentence:\n",
    "                    tagged_sentences.append(sentence)\n",
    "                    sentence = []\n",
    "                continue\n",
    "            tokens = line.split(\"\\t\")\n",
    "            sentence.append(TaggedWord(text=tokens[1], tag=tokens[3]))\n",
    "    return tagged_sentences\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "def write_tagged_sentence(tagged_sentence, f):\n",
    "    \"\"\"\n",
    "    Write tagged sentence in CoNLL-U format to file-like object f.\n",
    "    \"\"\"\n",
    "    for idx, word in enumerate(tagged_sentence):\n",
    "        line = [str(idx), word.text, \"_\", word.tag] + [\"_\"] * 6\n",
    "        f.write(\"\\t\".join(line) + \"\\n\")\n",
    "        \n",
    "\n",
    "def read_tags(path):\n",
    "    \"\"\"\n",
    "    Read a list of possible tags from file and return the list.\n",
    "    \"\"\"\n",
    "    tags = []\n",
    "    with open(path) as fin:\n",
    "        for line in fin:\n",
    "            tags.append(line.strip())\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II: evaluation\n",
    "\n",
    "We want you to estimate tagging quality by a simple accuracy: a fraction of tag predictions that turned out to be correct - averaged over the entire training corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types:\n",
    "TaggingQuality = collections.namedtuple('TaggingQuality', ['acc'])\n",
    "\n",
    "def tagging_quality(ref, out):\n",
    "    \"\"\"\n",
    "    Compute tagging quality and return TaggingQuality object.\n",
    "    \"\"\"\n",
    "    nwords = 0\n",
    "    ncorrect = 0\n",
    "    import itertools\n",
    "    for ref_sentence, out_sentence in itertools.zip_longest(ref, out):\n",
    "        for ref_word, out_word in itertools.zip_longest(ref_sentence, out_sentence):         \n",
    "            if ref_word and out_word and ref_word.tag == out_word.tag:\n",
    "                ncorrect += 1\n",
    "            nwords += 1\n",
    "    return ncorrect / nwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part III: Value and Update\n",
    "\n",
    "In order to implement two interlinked data structures: \n",
    "* __Value__ - a class that holds POS tagger's parameters. Basically an array of numbers\n",
    "* __Update__ - a class that stores updates for Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, n, random=False):\n",
    "        \"\"\"\n",
    "        Dense object that holds parameters.\n",
    "        :param n: array length\n",
    "        \"\"\"\n",
    "        self.values = np.random.normal(size=n) if random else np.zeros(n)\n",
    "\n",
    "    def dot(self, update):\n",
    "        result = 0\n",
    "        for pos, value in zip(update.positions, update.values):\n",
    "            result += self.values[pos] * value\n",
    "        return result\n",
    "\n",
    "    def assign(self, other):\n",
    "        \"\"\"\n",
    "        self = other\n",
    "        other is Value.\n",
    "        \"\"\"\n",
    "        self.values = other.values\n",
    "\n",
    "    def assign_mul(self, coeff):\n",
    "        \"\"\"\n",
    "        self = self * coeff\n",
    "        coeff is float.\n",
    "        \"\"\"\n",
    "        self.values *= coeff\n",
    "\n",
    "    def assign_madd(self, x, coeff):\n",
    "        \"\"\"\n",
    "        self = self + x * coeff\n",
    "        x can be either Value or Update.\n",
    "        coeff is float.\n",
    "        \"\"\"\n",
    "        x.assign_mul(coeff)\n",
    "        if isinstance(x, Value):\n",
    "            self.assign(x)\n",
    "        elif isinstance(x, Update):\n",
    "            for pos, value in zip(x.positions, x.values):\n",
    "                self.values[pos] += value\n",
    "        else:\n",
    "            raise ValueError(\"x can be either Value or Update.\")\n",
    "\n",
    "\n",
    "class Update:\n",
    "    \"\"\"\n",
    "    Sparse object that holds an update of parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, positions=None, values=None):\n",
    "        \"\"\"\n",
    "        positions: array of int\n",
    "        values: array of float\n",
    "        \"\"\"\n",
    "        self.positions = positions if positions is not None else np.array([])\n",
    "        self.values = values if values is not None else np.array([])\n",
    "\n",
    "    def assign_mul(self, coeff):\n",
    "        \"\"\"\n",
    "        self = self * coeff\n",
    "        coeff: float\n",
    "        \"\"\"\n",
    "        for idx, _ in enumerate(self.values):\n",
    "            self.values[idx] *= coeff\n",
    "\n",
    "    def assign_madd(self, update, coeff):\n",
    "        \"\"\"\n",
    "        self = self + update * coeff\n",
    "        coeff: float\n",
    "        \"\"\"\n",
    "        update.assign_mul(coeff)\n",
    "        pos_to_value = collections.defaultdict(float)\n",
    "        for pos, value in zip(self.positions, self.values):\n",
    "            pos_to_value[pos] += value\n",
    "        for pos, value in zip(update.positions, update.values):\n",
    "            pos_to_value[pos] += value\n",
    "        self.positions = []\n",
    "        self.values = []\n",
    "        for pos, value in pos_to_value.items():\n",
    "            if value:\n",
    "                self.positions.append(pos)\n",
    "                self.values.append(value)\n",
    "        self.positions = np.array(self.positions)\n",
    "        self.values = np.array(self.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part IV: Maximum Entropy POS Tagger\n",
    "_step 1 - draw an oval; step 2 - draw the rest of the owl (c)_\n",
    "\n",
    "In this secion you will implement a simple linear model to predict POS tags.\n",
    "Make sure you [read the article](W96-0213.pdf) before you proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nastya/.virtualenvs/nlp_pypy3/lib-python/3/importlib/_bootstrap.py:223: UserWarning: builtins.type size changed, may indicate binary incompatibility. Expected 872, got 416\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# Data Types:\n",
    "Features = Update\n",
    "Hypo = collections.namedtuple('Hypo', ['prev', 'pos', 'tagged_word', 'score'])\n",
    "# prev: previous Hypo\n",
    "# pos: position of word (0-based)\n",
    "# tagged_word: tagging of source_sentence[pos]\n",
    "# score: sum of scores over edges\n",
    "\n",
    "TaggerParams = collections.namedtuple('FeatureParams', [\n",
    "    'src_window',\n",
    "    'dst_order',\n",
    "    'max_suffix',\n",
    "    'beam_size',\n",
    "    'nparams'\n",
    "    ])\n",
    "\n",
    "import cityhash\n",
    "def h(x):\n",
    "    \"\"\"\n",
    "    Compute CityHash of any object.\n",
    "    Can be used to construct features.\n",
    "    \"\"\"\n",
    "    return cityhash.CityHash64(repr(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel:\n",
    "    \"\"\"\n",
    "    A thing that computes score and gradient for given features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n):\n",
    "        self._params = Value(n)\n",
    "\n",
    "    def params(self):\n",
    "        return self._params\n",
    "\n",
    "    def score(self, features):\n",
    "        \"\"\"\n",
    "        features: Update\n",
    "        \"\"\"\n",
    "        return self._params.dot(features)\n",
    "\n",
    "    def gradient(self, features, score):\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureComputer:\n",
    "    def __init__(self, tagger_params, source_sentence):\n",
    "        self.tagger_params = tagger_params\n",
    "        self.source_sentence = source_sentence\n",
    "        self.sent_len = len(source_sentence)\n",
    "\n",
    "    def compute_features(self, hypo):\n",
    "        \"\"\"\n",
    "        Compute features for a given Hypo and return Update.\n",
    "        \"\"\"\n",
    "        tagged_word = hypo.tagged_word\n",
    "        positions = [h(f\"word={tagged_word.text},tag={tagged_word.tag}\")]\n",
    "        for i in range(1, self.tagger_params.src_window + 1):\n",
    "            word_pos_minus_i = self.source_sentence[hypo.pos - i].text if hypo.pos - i >= 0 else \"None\"\n",
    "            word_pos_plus_i = (self.source_sentence[hypo.pos + i].text if hypo.pos + i < self.sent_len else \"None\")\n",
    "            feat_str_minus_i = f\"word_{-i}='{word_pos_minus_i}',tag='{tagged_word.tag}'\"\n",
    "            feat_str_plus_i = f\"word_{i}='{word_pos_plus_i}',tag='{tagged_word.tag}'\"\n",
    "            # print(feat_str_minus_i)\n",
    "            # print(feat_str_plus_i)\n",
    "            positions.append(h(feat_str_minus_i))\n",
    "            positions.append(h(feat_str_plus_i))\n",
    "        tag_sequence = []\n",
    "        current_hypo = hypo.prev if hypo else None\n",
    "        for i in range(1, self.tagger_params.dst_order):\n",
    "            tag_sequence.append(current_hypo.tagged_word.tag if current_hypo else \"None\")\n",
    "            tag_seq_str = \" \".join(tag_sequence)\n",
    "            # print(tag_seq_str)\n",
    "            positions.append(h(\n",
    "                f\"tag_seq_{i}='{tag_seq_str}',tag='{tagged_word.tag}'\"))\n",
    "            current_hypo = current_hypo.prev if current_hypo else None\n",
    "        for i in range(1, self.tagger_params.max_suffix + 1):\n",
    "            suffix_feat = f\"suff='{tagged_word.text[-i:]}',tag='{tagged_word.tag}'\"\n",
    "            prefix_feat = f\"pref='{tagged_word.text[:i]}',tag='{tagged_word.tag}'\"\n",
    "            # print(suffix_feat)\n",
    "            # print(prefix_feat)\n",
    "            positions.append(h(prefix_feat))\n",
    "            positions.append(h(suffix_feat))\n",
    "        positions = list(map(lambda x: x % self.tagger_params.nparams, positions))\n",
    "        return Update(positions=np.array(positions), values=np.ones(len(positions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedWord(text='From', tag='ADP'), TaggedWord(text='the', tag='DET'), TaggedWord(text='AP', tag='PROPN'), TaggedWord(text='comes', tag='VERB'), TaggedWord(text='this', tag='DET'), TaggedWord(text='story', tag='NOUN'), TaggedWord(text=':', tag='PUNCT')]\n",
      "[3327057   45182 1862397 1334518 3828493 3424278 1835926  799099   82873\n",
      " 1616455 2131331 2688935  498780 3750176 1722868]\n"
     ]
    }
   ],
   "source": [
    "sentence = read_tagged_sentences(\"data/en-ud-debug.conllu\")[0]\n",
    "print(sentence)\n",
    "params = TaggerParams(src_window=2, dst_order=3, max_suffix=4, nparams=2 ** 22, beam_size=1)\n",
    "computer = FeatureComputer(params, sentence)\n",
    "prev_hypo = Hypo(prev = None, pos=2, tagged_word=sentence[2], score=228)\n",
    "hypo = Hypo(prev=prev_hypo, pos=3, tagged_word=sentence[3], score=100500)\n",
    "print(computer.compute_features(hypo).positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part V: Beam search\n",
    "\n",
    "We can find the most likely tagging approximately using Beam Search. As everything else, it comes with a separate interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearchTask:\n",
    "    \"\"\"\n",
    "    An abstract beam search task. Can be used with beam_search() generic \n",
    "    function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tagger_params, source_sentence, model, tags):\n",
    "        self.tagger_params = tagger_params\n",
    "        self.source_sentence = source_sentence\n",
    "        self.model = model\n",
    "        self.tags = tags\n",
    "        self.feat_computer = FeatureComputer(tagger_params, source_sentence)\n",
    "\n",
    "    def total_num_steps(self):\n",
    "        \"\"\"\n",
    "        Number of hypotheses between beginning and end (number of words in\n",
    "        the sentence).\n",
    "        \"\"\"\n",
    "        return len(self.source_sentence)\n",
    "\n",
    "    def beam_size(self):\n",
    "        return self.tagger_params.beam_size\n",
    "            \n",
    "    def expand(self, hypo):\n",
    "        \"\"\"\n",
    "        Given Hypo, return a list of its possible expansions.\n",
    "        'hypo' might be None -- return a list of initial hypos then.\n",
    "\n",
    "        Compute hypotheses' scores inside this function!\n",
    "        \"\"\"\n",
    "        hypos = []\n",
    "        if hypo is not None and hypo.pos == self.total_num_steps() - 1:\n",
    "            return hypos\n",
    "        for tag in self.tags:\n",
    "            new_pos = hypo.pos + 1 if hypo is not None else 0\n",
    "            prev_score = hypo.score if hypo is not None else 0\n",
    "            hypo_to_score = Hypo(prev=hypo, pos=new_pos,\n",
    "                                 tagged_word=TaggedWord(text=self.source_sentence[new_pos].text,\n",
    "                                 tag=tag), score=0)\n",
    "            new_hypo_score = self.model.score(self.feat_computer.compute_features(hypo_to_score))\n",
    "            hypos.append(Hypo(prev=hypo, pos=new_pos,\n",
    "                              tagged_word=TaggedWord(text=self.source_sentence[new_pos].text,\n",
    "                              tag=tag),\n",
    "                              score=new_hypo_score + prev_score))\n",
    "        return hypos\n",
    "\n",
    "    def recombo_hash(self, hypo):\n",
    "        \"\"\"\n",
    "        If two hypos have the same recombination hashes, they can be collapsed\n",
    "        together, leaving only the hypothesis with a better score.\n",
    "        \"\"\"\n",
    "        tag_sequence = []\n",
    "        current_hypo = hypo\n",
    "        for i in range(self.tagger_params.dst_order):\n",
    "            tag_sequence.append(current_hypo.tagged_word.tag if current_hypo else \"None\")\n",
    "        return h(tag_sequence)\n",
    "        \n",
    "\n",
    "\n",
    "def beam_search(beam_search_task):\n",
    "    \"\"\"\n",
    "    Return list of stacks.\n",
    "    Each stack contains several hypos, sorted by score in descending \n",
    "    order (i.e. better hypos first).\n",
    "    \"\"\"\n",
    "    all_possible_hypos = beam_search_task.expand(None)\n",
    "    prev_stack = sorted(all_possible_hypos,\n",
    "                        key=lambda hypo: -hypo.score)[:beam_search_task.beam_size()]\n",
    "    stacks = [prev_stack]\n",
    "    for i in range(beam_search_task.total_num_steps() - 1):\n",
    "        proposed_hypos = []\n",
    "        for hypo in prev_stack:\n",
    "            proposed_hypos.extend(beam_search_task.expand(hypo))\n",
    "        hash_to_hypos = collections.defaultdict(list)\n",
    "        for hypo_ in proposed_hypos:\n",
    "            hash_to_hypos[beam_search_task.recombo_hash(hypo_)].append(hypo_)\n",
    "        best_hypos_without_copies = []\n",
    "        for _, equal_hash_hypos in hash_to_hypos.items():\n",
    "            best_hypo = equal_hash_hypos[0]\n",
    "            for hypo_ in equal_hash_hypos:\n",
    "                if best_hypo.score < hypo_.score:\n",
    "                    best_hypo = hypo_\n",
    "            best_hypos_without_copies.append(best_hypo)\n",
    "        prev_stack = sorted(best_hypos_without_copies,\n",
    "                            key=lambda hypo: -hypo.score)[:beam_search_task.beam_size()]\n",
    "        stacks.append(prev_stack)\n",
    "    return stacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_sentences(dataset, model, tags, tagger_params):\n",
    "    \"\"\"\n",
    "    Main predict function.\n",
    "    Tags all sentences in dataset. Dataset is a list of TaggedSentence; while \n",
    "    tagging, ignore existing tags.\n",
    "    \"\"\"\n",
    "    tagged_dataset = []\n",
    "    for idx, sentence in enumerate(dataset):     \n",
    "        beam_search_task = BeamSearchTask(tagger_params, sentence, model, tags)\n",
    "        hypos = []\n",
    "        stacks = beam_search(beam_search_task)\n",
    "        best_hypo = stacks[-1][0]\n",
    "        current_hypo = best_hypo\n",
    "        while current_hypo is not None:\n",
    "            hypos.append(current_hypo)\n",
    "            current_hypo = current_hypo.prev\n",
    "        tagged_sentence = []\n",
    "        for hypo in hypos[::-1]:\n",
    "            tagged_sentence.append(hypo.tagged_word)\n",
    "        tagged_dataset.append(tagged_sentence)\n",
    "        if idx == 0:\n",
    "            print(tagged_sentence)\n",
    "    return tagged_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part VI: Optimization objective and algorithm\n",
    "\n",
    "Once we defined our model and inference algorithm, we can define an optimization task: an object that computes loss function and its gradients w.r.t. model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizationTask:\n",
    "    \"\"\"\n",
    "    Optimization task that can be used with sgd().\n",
    "    \"\"\"\n",
    "\n",
    "    def params(self):\n",
    "        \"\"\"\n",
    "        Parameters which are optimized in this optimization task.\n",
    "        Return Value.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def loss_and_gradient(self, golden_sentence):\n",
    "        \"\"\"\n",
    "        Return (loss, gradient) on a specific example.\n",
    "\n",
    "        loss: float\n",
    "        gradient: Update\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class UnstructuredPerceptronOptimizationTask(OptimizationTask):\n",
    "    def __init__(self, tagger_params, tags):\n",
    "        self.model = LinearModel(tagger_params.nparams)\n",
    "        self.tagger_params = TaggerParams(src_window=tagger_params.src_window,\n",
    "                                          dst_order=tagger_params.dst_order,\n",
    "                                          max_suffix=tagger_params.max_suffix,\n",
    "                                          beam_size=1,\n",
    "                                          nparams=tagger_params.nparams)\n",
    "        self.tags = tags\n",
    "\n",
    "    def params(self):\n",
    "        return self.model.params()\n",
    "\n",
    "    def loss_and_gradient(self, golden_sentence):\n",
    "        beam_search_task = BeamSearchTask(\n",
    "        self.tagger_params, \n",
    "        golden_sentence,\n",
    "        # [golden_tagged_word.text for golden_tagged_word in golden_sentence], \n",
    "        self.model, \n",
    "        self.tags\n",
    "        )\n",
    "        stacks = beam_search(beam_search_task)\n",
    "        golden_hypos = []\n",
    "        golden_hypo = None\n",
    "        feature_computer = FeatureComputer(self.tagger_params, golden_sentence)\n",
    "        for i in range(len(golden_sentence)):\n",
    "            new_golden_hypo = Hypo(prev=golden_hypo, pos=i,\n",
    "                                   tagged_word=golden_sentence[i],\n",
    "                                   score=0)\n",
    "            new_hypo_score = self.model.score(feature_computer.compute_features(new_golden_hypo))\n",
    "            new_golden_hypo = Hypo(prev=golden_hypo, pos=i,\n",
    "                                   tagged_word=golden_sentence[i],\n",
    "                                   score=new_hypo_score)\n",
    "            golden_hypo = new_golden_hypo\n",
    "            golden_hypos.append(golden_hypo)\n",
    "        golden_head = golden_hypos[-1]\n",
    "        rival_head = stacks[-1][0]\n",
    "        grad = Update()\n",
    "        loss = 0\n",
    "        while golden_head and rival_head:\n",
    "            rival_features = feature_computer.compute_features(rival_head)\n",
    "            grad.assign_madd(self.model.gradient(rival_features, score=None), 1)\n",
    "\n",
    "            golden_features = feature_computer.compute_features(golden_head)\n",
    "            grad.assign_madd(self.model.gradient(golden_features, score=None), -1)\n",
    "            loss += rival_head.score - golden_head.score\n",
    "            golden_head = golden_head.prev\n",
    "            rival_head = rival_head.prev\n",
    "        return loss, grad\n",
    "        \n",
    "\n",
    "\n",
    "class StructuredPerceptronOptimizationTask(OptimizationTask):\n",
    "    def __init__(self, tagger_params, tags):\n",
    "        self.tagger_params = tagger_params\n",
    "        self.model = LinearModel(self.tagger_params.nparams)\n",
    "        self.tags = tags\n",
    "\n",
    "    def params(self):\n",
    "        return self.model.params()\n",
    "\n",
    "    def loss_and_gradient(self, golden_sentence):\n",
    "        # Do beam search.\n",
    "        beam_search_task = BeamSearchTask(\n",
    "            self.tagger_params, \n",
    "            golden_sentence,\n",
    "            # [golden_tagged_word.text for golden_tagged_word in golden_sentence], \n",
    "            self.model, \n",
    "            self.tags\n",
    "            )\n",
    "        stacks = beam_search(beam_search_task)\n",
    "\n",
    "        # Compute chain of golden hypos (and their scores!).\n",
    "        golden_hypos = []\n",
    "        golden_hypo = None\n",
    "        feature_computer = FeatureComputer(self.tagger_params, golden_sentence)\n",
    "        for i in range(len(golden_sentence)):\n",
    "            new_golden_hypo = Hypo(prev=golden_hypo, pos=i,\n",
    "                                   tagged_word=golden_sentence[i],\n",
    "                                   score=0)\n",
    "            new_hypo_score = self.model.score(feature_computer.compute_features(new_golden_hypo))\n",
    "            new_golden_hypo = Hypo(prev=golden_hypo, pos=i,\n",
    "                                   tagged_word=golden_sentence[i],\n",
    "                                   score=new_hypo_score)\n",
    "            golden_hypo = new_golden_hypo\n",
    "            golden_hypos.append(golden_hypo)\n",
    "\n",
    "        # Find where to update.\n",
    "        golden_head = None\n",
    "        rival_head = None\n",
    "        for idx, hypo in enumerate(golden_hypos):\n",
    "            if hypo.score < stacks[idx][-1].score:\n",
    "                golden_head = hypo\n",
    "                rival_head = stacks[idx][0]\n",
    "        if golden_head is None and rival_head is None:\n",
    "            golden_head = golden_hypos[-1] \n",
    "            rival_head = stacks[-1][0]\n",
    "        # Compute gradient.\n",
    "        grad = Update()\n",
    "        loss = 0\n",
    "        while golden_head and rival_head:\n",
    "            rival_features = feature_computer.compute_features(rival_head)\n",
    "            grad.assign_madd(self.model.gradient(rival_features, score=None), 1)\n",
    "\n",
    "            golden_features = feature_computer.compute_features(golden_head)\n",
    "            grad.assign_madd(self.model.gradient(golden_features, score=None), -1)\n",
    "            loss += rival_head.score - golden_head.score\n",
    "            golden_head = golden_head.prev\n",
    "            rival_head = rival_head.prev\n",
    "\n",
    "        return loss, grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part VII: optimizer\n",
    "\n",
    "By this point we can define a model with parameters $\\theta$ and a problem that computes gradients $ \\partial L \\over \\partial \\theta $ w.r.t. model parameters.\n",
    "\n",
    "Optimization is performed by gradient descent: $ \\theta := \\theta - \\alpha {\\partial L \\over \\partial \\theta} $\n",
    "\n",
    "In order to speed up training, we use stochastic gradient descent that operates on minibatches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGDParams = collections.namedtuple('SGDParams', [\n",
    "    'epochs',\n",
    "    'learning_rate',\n",
    "    'minibatch_size',\n",
    "    'average' # bool or int\n",
    "    ])\n",
    "\n",
    "\n",
    "def make_batches(dataset, minibatch_size):\n",
    "    \"\"\"\n",
    "    Make list of batches from a list of examples.\n",
    "    \"\"\"\n",
    "    batches = []\n",
    "    batch = []\n",
    "    shuffled_dataset = np.random.permutation(dataset)\n",
    "    for sentence in shuffled_dataset:\n",
    "        batch.append(sentence)\n",
    "        if len(batch) % minibatch_size == 0:\n",
    "            batches.append(batch)\n",
    "            batch = []\n",
    "    if len(batch) > 0:\n",
    "        batches.append(batch)\n",
    "    return batches\n",
    "\n",
    "\n",
    "def sgd(sgd_params, optimization_task, dataset, after_each_epoch_fn):\n",
    "    \"\"\"\n",
    "    Run (averaged) SGD on a generic optimization task. Modify optimization\n",
    "    task's parameters.\n",
    "\n",
    "    After each epoch (and also before and after the whole training),\n",
    "    run after_each_epoch_fn().\n",
    "    \"\"\"\n",
    "    after_each_epoch_fn()\n",
    "    all_params_avg = optimization_task.params()\n",
    "    for epoch in range(sgd_params.epochs):\n",
    "        start = time.time()\n",
    "        for batch in make_batches(dataset, sgd_params.minibatch_size):\n",
    "            average_grad = Update()\n",
    "            average_loss = 0\n",
    "            for sentence in batch:\n",
    "                loss, grad = optimization_task.loss_and_gradient(sentence)\n",
    "                average_grad.assign_madd(grad, 1)\n",
    "                average_loss += loss\n",
    "            average_grad.assign_mul(1 / sgd_params.minibatch_size)\n",
    "            average_loss /= sgd_params.minibatch_size\n",
    "            optimization_task.params().assign_madd(average_grad, -sgd_params.learning_rate)\n",
    "            all_params_avg.assign_madd(optimization_task.params(), 1)\n",
    "        after_each_epoch_fn()\n",
    "        print(f\"Epoch time: {time.time() - start} s\")\n",
    "    all_params_avg.assign_mul(1 / (sgd_params.epochs * int(len(dataset) / sgd_params.minibatch_size + 1)))\n",
    "    if sgd_params.average:\n",
    "        optimization_task.params().assign(all_params_avg)\n",
    "    after_each_epoch_fn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part VIII: Training loop\n",
    "\n",
    "The train function combines everthing you used below to get new "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    tags_path='./data/tags',\n",
    "    train_dataset='./data/en-ud-train.conllu',\n",
    "    dev_dataset='./data/en-ud-dev.conllu',\n",
    "    model_path='./model.npz',\n",
    "    \n",
    "    sgd_epochs=15,\n",
    "    sgd_learning_rate=0.02,\n",
    "    sgd_minibatch_size=32,\n",
    "    sgd_average=True,\n",
    "    \n",
    "    # Number of context tags in output tagging to use for features\n",
    "    tagger_src_window=2,\n",
    "    \n",
    "    # Number of context tags in output tagging to use for features\n",
    "    tagger_dst_order=3,\n",
    "    \n",
    "    # Maximal number of prefix/suffix letters to use for features\n",
    "    tagger_max_suffix=4,\n",
    "    \n",
    "    # Width for beam search (0 means unstructured)\n",
    "    beam_size=1,\n",
    "    \n",
    "    # Parameter vector size (for hashing)\n",
    "    nparams= 2 ** 22,\n",
    "):\n",
    "    \"\"\" Train a pos-tagger model and save it's parameters to :model: \"\"\"\n",
    "\n",
    "    # Beam size.\n",
    "    optimization_task_cls = StructuredPerceptronOptimizationTask\n",
    "    if beam_size == 0:\n",
    "        beam_size = 1\n",
    "        optimization_task_cls = UnstructuredPerceptronOptimizationTask\n",
    "\n",
    "    # Parse cmdargs.\n",
    "    tags = read_tags(tags_path)\n",
    "    train_dataset = read_tagged_sentences(train_dataset)\n",
    "    dev_dataset = read_tagged_sentences(dev_dataset)\n",
    "    params = None\n",
    "    if os.path.exists(model_path):\n",
    "        params = pickle.load(open(model_path, 'rb'))\n",
    "    sgd_params = SGDParams(\n",
    "        epochs=sgd_epochs,\n",
    "        learning_rate=sgd_learning_rate,\n",
    "        minibatch_size=sgd_minibatch_size,\n",
    "        average=sgd_average\n",
    "        )\n",
    "    tagger_params = TaggerParams(\n",
    "        src_window=tagger_src_window,\n",
    "        dst_order=tagger_dst_order,\n",
    "        max_suffix=tagger_max_suffix,\n",
    "        beam_size=beam_size,\n",
    "        nparams=nparams\n",
    "        )\n",
    "\n",
    "    # Load optimization task\n",
    "    optimization_task = optimization_task_cls(tagger_params, tags)\n",
    "    if params is not None:\n",
    "        print('\\n\\nLoading parameters from %s\\n\\n' % model_path)\n",
    "        optimization_task.params().assign(params)\n",
    "\n",
    "    # Validation.\n",
    "    def after_each_epoch_fn():\n",
    "        model = LinearModel(nparams)\n",
    "        model.params().assign(optimization_task.params())\n",
    "        tagged_sentences = tag_sentences(dev_dataset, model, tags, tagger_params)\n",
    "        q = pprint.pformat(tagging_quality(out=tagged_sentences, ref=dev_dataset))\n",
    "        print()\n",
    "        print(q)\n",
    "        print()\n",
    "\n",
    "        # Save parameters.\n",
    "        print('\\n\\nSaving parameters to %s\\n\\n' % model_path)\n",
    "        pickle.dump(optimization_task.params(), open(model_path, 'wb'))\n",
    "\n",
    "    # Run SGD.\n",
    "    sgd(sgd_params, optimization_task, train_dataset, after_each_epoch_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading parameters from ./default_model_6.npz\n",
      "\n",
      "\n",
      "[TaggedWord(text='From', tag='NOUN'), TaggedWord(text='the', tag='NOUN'), TaggedWord(text='AP', tag='NOUN'), TaggedWord(text='comes', tag='NOUN'), TaggedWord(text='this', tag='NOUN'), TaggedWord(text='story', tag='NOUN'), TaggedWord(text=':', tag='NOUN')]\n",
      "\n",
      "0.16280971461280563\n",
      "\n",
      "\n",
      "\n",
      "Saving parameters to ./default_model_6.npz\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-2c1460faf938>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train a model with default params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./default_model_6.npz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-1e65d1c4a3d0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(tags_path, train_dataset, dev_dataset, model_path, sgd_epochs, sgd_learning_rate, sgd_minibatch_size, sgd_average, tagger_src_window, tagger_dst_order, tagger_max_suffix, beam_size, nparams)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m# Run SGD.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0msgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msgd_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimization_task\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mafter_each_epoch_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-9829494b7a48>\u001b[0m in \u001b[0;36msgd\u001b[0;34m(sgd_params, optimization_task, dataset, after_each_epoch_fn)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0maverage_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimization_task\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_and_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m                 \u001b[0maverage_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0maverage_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-44666db0975c>\u001b[0m in \u001b[0;36mloss_and_gradient\u001b[0;34m(self, golden_sentence)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         )\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mstacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeam_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeam_search_task\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mgolden_hypos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mgolden_hypo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-b2d1b1e2238d>\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(beam_search_task)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mproposed_hypos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhypo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprev_stack\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mproposed_hypos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeam_search_task\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mhash_to_hypos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhypo_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproposed_hypos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-b2d1b1e2238d>\u001b[0m in \u001b[0;36mexpand\u001b[0;34m(self, hypo)\u001b[0m\n\u001b[1;32m     38\u001b[0m                                  tagged_word=TaggedWord(text=self.source_sentence[new_pos].text,\n\u001b[1;32m     39\u001b[0m                                  tag=tag), score=0)\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mnew_hypo_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeat_computer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypo_to_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             hypos.append(Hypo(prev=hypo, pos=new_pos,\n\u001b[1;32m     42\u001b[0m                               tagged_word=TaggedWord(text=self.source_sentence[new_pos].text,\n",
      "\u001b[0;32m<ipython-input-7-88acfdd31b12>\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUpdate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \"\"\"\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-dfcd2011cdb7>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(self, update)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train a model with default params\n",
    "train(model_path='./default_model.npz', beam_size=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part IX: Evaluate the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(\n",
    "    tags='./data/tags',\n",
    "    dataset='./data/en-ud-dev.conllu',\n",
    "    model='./model.npz',\n",
    "    \n",
    "    # model and inference params; see train for their description\n",
    "    tagger_src_window=2,\n",
    "    tagger_dst_order=3,\n",
    "    tagger_max_suffix=4,\n",
    "    beam_size=1,\n",
    "):\n",
    "\n",
    "\n",
    "    tags = read_tags(tags)\n",
    "    dataset = read_tagged_sentences(dataset)\n",
    "    params = pickle.load(open(model, 'rb'))\n",
    "    tagger_params = TaggerParams(\n",
    "        src_window=tagger_src_window,\n",
    "        dst_order=tagger_dst_order,\n",
    "        max_suffix=tagger_max_suffix,\n",
    "        beam_size=beam_size,\n",
    "        nparams=0\n",
    "        )\n",
    "\n",
    "    # Load model.\n",
    "    model = LinearModel(params.values.shape[0])\n",
    "    model.params().assign(params)\n",
    "\n",
    "    # Tag all sentences.\n",
    "    tagged_sentences = tag_sentences(dataset, model, tags, tagger_params)\n",
    "\n",
    "    # Write tagged sentences.\n",
    "    for tagged_sentence in tagged_sentences:\n",
    "        write_tagged_sentence(tagged_sentence, sys.stdout)\n",
    "\n",
    "    # Measure and print quality.\n",
    "    q = pprint.pformat(tagging_quality(out=tagged_sentences, ref=dataset))\n",
    "    print(q, file=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test \n",
    "test(model='./default_model.npz')\n",
    "\n",
    "# sanity check: accuracy > 90%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part X: play with it\n",
    "\n",
    "_This part is optional_\n",
    "\n",
    "Once you've built something, it's only natural to test the limits of your contraption.\n",
    "\n",
    "At minumum, we want you to find out how default model accuracy depends on __beam size__\n",
    "\n",
    "To get maximum points, your model should get final quality >= 93% \n",
    "\n",
    "Any further analysis is welcome, as always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<YOUR CODE>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
