{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Word Alignment Assignment__\n",
    "\n",
    "Your task is to learn word alignments for the data provided with this Python Notebook. \n",
    "\n",
    "Start by running the 'train' function below and implementing the assertions which will fail. Then consider the following improvements to the baseline model:\n",
    "* Is the TranslationModel parameterized efficiently?\n",
    "* What form of PriorModel would help here? (Currently the PriorModel is uniform.)\n",
    "* How could you use a Hidden Markov Model to model word alignment indices? (There's an implementation of simple HMM below to help you start.)\n",
    "* How could you initialize more complex models from simpler ones?\n",
    "* How could you model words that are not aligned to anything?\n",
    "\n",
    "Grades will be assigned as follows*:\n",
    "\n",
    " AER below on blinds   |  Grade \n",
    "----------|-------------\n",
    " 0.5 - 0.6 |   1 \n",
    " 0.4 - 0.5 |   2 \n",
    " 0.35 - 0.4 |  3    \n",
    " 0.3 - 0.35 |  4    \n",
    " 0.25 - 0.3 |  5   \n",
    " \n",
    "You should save the notebook with the final scores for 'dev' and 'test' test sets.\n",
    "\n",
    "*__Note__: Students who submitted a version of this assignment last year will have a 0.05 AER handicap, i.e to get a grade of 5, they will need to get an AER below 0.25.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell contains the generative models that you may want to use for word alignment.\n",
    "# Currently only the TranslationModel is at all functional.\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "\n",
    "class TranslationModel:\n",
    "    \"Models conditional distribution over trg words given a src word.\"\n",
    "\n",
    "    def __init__(self, src_corpus, trg_corpus, identity_matrix, hard_align=False):\n",
    "        self.identity_matrix = identity_matrix\n",
    "        self.num_unique_src_tokens = identity_matrix.shape[0]\n",
    "        self.num_unique_trg_tokens = identity_matrix.shape[1]\n",
    "        self._trg_given_src_probs = np.ones((self.num_unique_src_tokens,\n",
    "                                             self.num_unique_trg_tokens)) / self.num_unique_trg_tokens\n",
    "        self._src_trg_counts = np.zeros((self.num_unique_src_tokens, self.num_unique_trg_tokens))\n",
    "        self.hard_align = hard_align\n",
    "\n",
    "    def get_params(self):\n",
    "        return self._trg_given_src_probs\n",
    "\n",
    "    def get_conditional_prob(self, src_token, trg_token):\n",
    "        \"Return the conditional probability of trg_token given src_token.\"\n",
    "        return self._trg_given_src_probs[src_token][trg_token]\n",
    "\n",
    "    def get_parameters_for_sentence_pair(self, src_tokens, trg_tokens):\n",
    "        \"Returns matrix with t[i][j] = p(f_j|e_i).\"\n",
    "        return self._trg_given_src_probs[np.ix_(src_tokens, trg_tokens)]\n",
    "\n",
    "    def collect_statistics(self, src_tokens, trg_tokens, posterior_matrix, hmm=False):\n",
    "        \"Accumulate counts of translations from: posterior_matrix[j][i] = p(a_j=i|e, f)\"\n",
    "#         assert posterior_matrix.shape == (len(trg_tokens), len(src_tokens))\n",
    "        # assert False, \"Implement collection of statistics here.\"\n",
    "        self._src_trg_counts[np.ix_(src_tokens, trg_tokens)] += posterior_matrix\n",
    "       \n",
    "        \n",
    "    def recompute_parameters(self):\n",
    "        \"Reestimate parameters and reset counters.\"\n",
    "        # assert False, \"Implement reestimation of parameters from counters here.\"\n",
    "        self._trg_given_src_probs = self._src_trg_counts / np.sum(self._src_trg_counts, axis=1, keepdims=True)\n",
    "        self._src_trg_counts = np.zeros((self.num_unique_src_tokens, self.num_unique_trg_tokens))\n",
    "        if self.hard_align:\n",
    "            self._trg_given_src_probs[self.identity_matrix.row, self.identity_matrix.col] = 1.0\n",
    "\n",
    "\n",
    "class PriorModel:\n",
    "    \"Models the prior probability of an alignment given only the sentence lengths and token indices.\"\n",
    "\n",
    "    def __init__(self, src_corpus, trg_corpus):\n",
    "        \"Add counters and parameters here for more sophisticated models.\"\n",
    "        self._distance_counts = {}\n",
    "        self._distance_probs = {}\n",
    "\n",
    "    def get_parameters_for_sentence_pair(self, src_tokens, trg_tokens):\n",
    "        src_length = len(src_tokens)\n",
    "        trg_length = len(trg_tokens)\n",
    "        return np.ones((src_length, trg_length)) * 1.0 / src_length\n",
    "    \n",
    "    def get_prior_prob(self, src_index, trg_index, src_length, trg_length):\n",
    "        \"Returns a uniform prior probability.\"\n",
    "        return 1.0 / src_length\n",
    "\n",
    "    def collect_statistics(self, src_length, trg_length, posterior_matrix):\n",
    "        \"Extract the necessary statistics from this matrix if needed.\"\n",
    "        pass\n",
    "\n",
    "    def recompute_parameters(self):\n",
    "        \"Reestimate the parameters and reset counters.\"\n",
    "        pass\n",
    "    \n",
    "    \n",
    "class ComplexPriorModel:\n",
    "    \"Models the prior probability of an alignment given the sentence lengths and token indices.\"\n",
    "\n",
    "    def __init__(self, src_corpus, trg_corpus, use_null=False,\n",
    "                 src_phi=0.5, trg_phi=0.5, src_null_index=0, trg_null_index=0):\n",
    "        \"Add counters and parameters here for more sophisticated models.\"\n",
    "        self.num_src_indices = np.max(list(map(len, src_corpus)))\n",
    "        self.num_trg_indices = np.max(list(map(len, trg_corpus)))\n",
    "        self._distance_counts = defaultdict(lambda: \n",
    "                                            np.zeros((self.num_src_indices,\n",
    "                                                      self.num_trg_indices)))\n",
    "        self._distance_probs = defaultdict(lambda:\n",
    "                                           np.ones((self.num_src_indices,\n",
    "                                                    self.num_trg_indices)) / self.num_trg_indices)\n",
    "        self.src_phi = src_phi\n",
    "        self.trg_phi = trg_phi\n",
    "        self.src_null_index = src_null_index\n",
    "        self.trg_null_index = trg_null_index\n",
    "        self.use_null = use_null\n",
    "\n",
    "    def get_prior_prob(self, src_index, trg_index, src_length, trg_length):\n",
    "        \"Returns a uniform prior probability.\"\n",
    "        return self._distance_probs[(src_length, trg_length)][src_index, trg_index]\n",
    "\n",
    "    def get_parameters_for_sentence_pair(self, src_tokens, trg_tokens):\n",
    "        src_length = len(src_tokens)\n",
    "        trg_length = len(trg_tokens)\n",
    "        return (self._distance_probs[(src_length, trg_length)]\n",
    "                [np.ix_(np.arange(src_length), np.arange(trg_length))])\n",
    "\n",
    "    def collect_statistics(self, src_tokens, trg_tokens, posterior_matrix):\n",
    "        \"Extract the necessary statistics from this matrix if needed.\"\n",
    "        src_length = len(src_tokens)\n",
    "        trg_length = len(trg_tokens)\n",
    "        src_indices = np.arange(src_length)\n",
    "        trg_indices = np.arange(trg_length)\n",
    "        (self._distance_counts[(src_length, trg_length)]\n",
    "         [np.ix_(src_indices, trg_indices)]) += posterior_matrix\n",
    "\n",
    "    def recompute_parameters(self):\n",
    "        \"Reestimate the parameters and reset counters.\"\n",
    "        for key in self._distance_counts:\n",
    "            denoms = np.sum(self._distance_counts[key], axis=0, keepdims=True)\n",
    "            self._distance_probs[key] = self._distance_counts[key] / denoms\n",
    "            if self.use_null:\n",
    "                self._distance_probs[key][self.src_null_index, :] *= self.src_phi\n",
    "                self._distance_probs[key][:self.src_null_index, :] *= (1 - self.src_phi)\n",
    "                self._distance_probs[key][(self.src_null_index + 1):, :] *= (1 - self.src_phi)\n",
    "                self._distance_probs[key][:, self.trg_null_index] *= self.trg_phi\n",
    "                self._distance_probs[key][:, :self.trg_null_index] *= (1 - self.trg_phi)\n",
    "                self._distance_probs[key][:, (self.trg_null_index + 1):] *= (1 - self.trg_phi)\n",
    "            self._distance_counts[key] = np.zeros((self.num_src_indices, self.num_trg_indices))\n",
    "        \n",
    "\n",
    "class ImprovedComplexPriorModel:\n",
    "    \"Models the prior probability of an alignment given the sentence lengths and token indices.\"\n",
    "\n",
    "    def __init__(self, src_corpus, trg_corpus, num_indices=10,\n",
    "                 use_null=False, src_phi=0.5, trg_phi=0.5, src_null_index=0, trg_null_index=0):\n",
    "        \"Add counters and parameters here for more sophisticated models.\"\n",
    "        self.num_src_indices = num_indices\n",
    "        self.num_trg_indices = num_indices\n",
    "        self._distance_counts = np.zeros((self.num_src_indices, self.num_trg_indices))\n",
    "        self._distance_probs = np.ones((self.num_src_indices,\n",
    "                                        self.num_trg_indices)) / self.num_trg_indices\n",
    "        self.src_phi = src_phi\n",
    "        self.trg_phi = trg_phi\n",
    "        self.src_null_index = src_null_index\n",
    "        self.trg_null_index = trg_null_index\n",
    "        self.use_null = use_null\n",
    "\n",
    "    def get_prior_prob(self, src_index, trg_index, src_length, trg_length):\n",
    "        \"Returns a uniform prior probability.\"\n",
    "        return self._distance_probs[int(trg_index / trg_length * self.num_trg_indices),\n",
    "                                    int(src_index / src_length * self.num_src_indices)]\n",
    "    \n",
    "    def get_parameters_for_sentence_pair(self, src_tokens, trg_tokens):\n",
    "        src_length = len(src_tokens)\n",
    "        trg_length = len(trg_tokens)\n",
    "        squeezed_src_indices = np.array(list(map(lambda x: int(x / src_length * self.num_src_indices),\n",
    "                                        np.arange(src_length))))\n",
    "        squeezed_trg_indices = np.array(list(map(lambda x: int(x / trg_length * self.num_trg_indices),\n",
    "                                np.arange(trg_length))))\n",
    "        return self._distance_probs[np.ix_(squeezed_src_indices, squeezed_trg_indices)]\n",
    "\n",
    "    def collect_statistics(self, src_tokens, trg_tokens, posterior_matrix):\n",
    "        \"Extract the necessary statistics from this matrix if needed.\"\n",
    "        src_length = len(src_tokens)\n",
    "        trg_length = len(trg_tokens)\n",
    "        squeezed_src_indices = np.array(list(map(lambda x: int(x / src_length * self.num_src_indices),\n",
    "                                        np.arange(src_length))))\n",
    "        squeezed_trg_indices = np.array(list(map(lambda x: int(x / trg_length * self.num_trg_indices),\n",
    "                                np.arange(trg_length))))\n",
    "        self._distance_counts[np.ix_(squeezed_src_indices, squeezed_trg_indices)] += posterior_matrix\n",
    "\n",
    "    def recompute_parameters(self):\n",
    "        \"Reestimate the parameters and reset counters.\"\n",
    "        denoms = np.sum(self._distance_counts, axis=0, keepdims=True)\n",
    "        self._distance_probs = self._distance_counts / denoms\n",
    "        if self.use_null:\n",
    "            self._distance_probs[self.src_null_index, :] *= self.src_phi\n",
    "            self._distance_probs[:self.src_null_index, :] *= (1 - self.src_phi)\n",
    "            self._distance_probs[(self.src_null_index + 1):, :] *= (1 - self.src_phi)\n",
    "            self._distance_probs[:, self.trg_null_index] *= self.trg_phi\n",
    "            self._distance_probs[:, :self.trg_null_index] *= (1 - self.trg_phi)\n",
    "            self._distance_probs[:, (self.trg_null_index + 1):] *= (1 - self.trg_phi)\n",
    "        self._distance_counts = np.zeros((self.num_src_indices, self.num_trg_indices))\n",
    "\n",
    "class TransitionModel:\n",
    "    \"Models the prior probability of an alignment conditioned on previous alignment.\"\n",
    "\n",
    "    def __init__(self, src_corpus, trg_corpus):\n",
    "        \"Add counters and parameters here for more sophisticated models.\"\n",
    "        self.num_src_indices = np.max(list(map(len, src_corpus)))\n",
    "        self.alignment_probs_given_prev = dict()\n",
    "        self.alignment_counts = dict()\n",
    "\n",
    "    def get_parameters_for_sentence_pair(self, src_tokens, trg_tokens):\n",
    "        \"Retrieve the parameters for this sentence pair: A[k, i] = p(a_{j} = i|a_{j-1} = k)\"\n",
    "        src_length = len(src_tokens)\n",
    "        trg_length = len(trg_tokens)\n",
    "        if src_length not in self.alignment_probs_given_prev:\n",
    "            self.alignment_probs_given_prev[src_length] = np.ones((src_length, src_length)) / src_length\n",
    "        return self.alignment_probs_given_prev[src_length]\n",
    "                \n",
    "    def collect_statistics(self, src_tokens, trg_tokens, bigram_posteriors):\n",
    "        \"Extract statistics from the bigram posterior[i][j]: p(a_{t-1} = i, a_{t} = j| e, f)\"\n",
    "        src_length = len(src_tokens)\n",
    "        trg_length = len(trg_tokens)\n",
    "        if src_length not in self.alignment_counts:\n",
    "            self.alignment_counts[src_length] = np.zeros((src_length, src_length))\n",
    "        self.alignment_counts[src_length] += np.sum(bigram_posteriors, axis=2)\n",
    "        \n",
    "        \n",
    "    def recompute_parameters(self):\n",
    "        \"Recompute the transition matrix\"\n",
    "        for length in self.alignment_counts:\n",
    "            denoms = np.sum(self.alignment_counts[length], axis=0, keepdims=True)\n",
    "            self.alignment_probs_given_prev[length] = self.alignment_counts[length] / denoms\n",
    "            self.alignment_counts[length] = np.zeros((length, length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell contains the framework for training and evaluating a model using EM.\n",
    "\n",
    "from utils import read_parallel_corpus, extract_test_set_alignments, score_alignments\n",
    "from itertools import starmap\n",
    "from math import log\n",
    "from scipy.sparse import coo_matrix\n",
    "import editdistance\n",
    "import multiprocessing\n",
    "import os\n",
    "import functools\n",
    "\n",
    "\n",
    "def infer_posteriors(src_tokens, trg_tokens, prior_model, translation_model, hmm=False):\n",
    "    \"Compute the posterior probability p(a_j=i | f, e) for each target token f_j given e and f.\"\n",
    "    # HINT: An HMM will require more complex statistics over the hidden alignments.\n",
    "    P = prior_model.get_parameters_for_sentence_pair(src_tokens, trg_tokens)\n",
    "    T = translation_model.get_parameters_for_sentence_pair(src_tokens, trg_tokens) # t[i][j] = P(f_j|e_i)\n",
    "    # assert False, \"Compute the posterior distribution over src indices for each trg word.\"\n",
    "    if hmm:\n",
    "        initial_distribution = np.ones(len(src_tokens)) / len(src_tokens)\n",
    "        bigram_posterior_matrix = np.zeros((len(src_tokens), len(src_tokens), len(trg_tokens)))\n",
    "        unigram_posterior_matrix = np.zeros((len(trg_tokens), len(src_tokens)))\n",
    "        alpha, beta, sentence_marginal_log_likelihood = forward_backward(initial_distribution, P, T)\n",
    "        \n",
    "        unigram_posterior_matrix = alpha * beta\n",
    "        denoms = np.sum(unigram_posterior_matrix, axis=0, keepdims=True)\n",
    "        unigram_posterior_matrix /= denoms\n",
    "        \n",
    "        bigram_posterior_matrix = (alpha[:, None, :-1] * P[:, :, None] *\n",
    "                                   beta[None, :, 1:] * T[None, :, 1:])\n",
    "        denoms = np.sum(bigram_posterior_matrix, axis=(0, 1), keepdims=True)\n",
    "        bigram_posterior_matrix /= denoms\n",
    "        return unigram_posterior_matrix, bigram_posterior_matrix, sentence_marginal_log_likelihood\n",
    "    posterior_matrix = P * T\n",
    "    denoms = np.sum(posterior_matrix, axis=0, keepdims=True)\n",
    "    posterior_matrix /= denoms\n",
    "    sentence_marginal_log_likelihood = np.sum(np.log(denoms))\n",
    "    return posterior_matrix, sentence_marginal_log_likelihood\n",
    "\n",
    "def collect_expected_statistics(src_corpus, trg_corpus, prior_model, translation_model, hmm=False):\n",
    "    \"E-step: infer posterior distribution over each sentence pair and collect statistics.\"\n",
    "    corpus_log_likelihood = 0.0\n",
    "    for src_tokens, trg_tokens in zip(src_corpus, trg_corpus):\n",
    "        # Infer posterior\n",
    "        if hmm:\n",
    "            unigram_posteriors, bigram_posteriors, log_likelihood = infer_posteriors(\n",
    "                src_tokens, trg_tokens, prior_model, translation_model, hmm=hmm)\n",
    "            prior_model.collect_statistics(src_tokens, trg_tokens, bigram_posteriors)\n",
    "            translation_model.collect_statistics(src_tokens, trg_tokens, unigram_posteriors)\n",
    "        else:\n",
    "            posteriors, log_likelihood = infer_posteriors(src_tokens, trg_tokens, prior_model,\n",
    "                                                          translation_model, hmm=hmm)\n",
    "            # Collect statistics in each model.\n",
    "            prior_model.collect_statistics(src_tokens, trg_tokens, posteriors)\n",
    "            translation_model.collect_statistics(src_tokens, trg_tokens, posteriors)\n",
    "        # Update log prob\n",
    "        corpus_log_likelihood += log_likelihood\n",
    "    return corpus_log_likelihood\n",
    "\n",
    "def estimate_models(src_corpus, trg_corpus, prior_model, translation_model,\n",
    "                    num_iterations, hmm=False, use_null=False,\n",
    "                    src_null_index=0, trg_null_index=0):\n",
    "    \"Estimate models iteratively using EM.\"\n",
    "    for iteration in range(num_iterations):\n",
    "        # E-step\n",
    "        corpus_log_likelihood = collect_expected_statistics(src_corpus, trg_corpus,\n",
    "                                                            prior_model, translation_model, hmm=hmm)\n",
    "        # M-step\n",
    "        prior_model.recompute_parameters()\n",
    "        translation_model.recompute_parameters()\n",
    "        if iteration > 0:\n",
    "            print(\"corpus log likelihood: %1.3f\" % corpus_log_likelihood)\n",
    "            aligned_corpus = align_corpus(src_corpus, trg_corpus,\n",
    "                                          prior_model, translation_model, hmm=hmm,\n",
    "                                          use_null=use_null, src_null_index=src_null_index,\n",
    "                                          trg_null_index=trg_null_index)\n",
    "            evaluate(extract_test_set_alignments(aligned_corpus))\n",
    "    return prior_model, translation_model\n",
    "\n",
    "\n",
    "def get_alignments_from_posterior(posteriors, hmm=False, use_null=False,\n",
    "                                  src_null_index=0, trg_null_index=0):\n",
    "    \"Returns the MAP alignment for each target word given the posteriors.\"\n",
    "    # HINT: If you implement an HMM, you may want to implement a better algorithm here.\n",
    "    alignments = {}\n",
    "    for trg_index, src_index in enumerate(np.argmax(posteriors, 0)):\n",
    "        if src_index == src_null_index or trg_index == trg_null_index:\n",
    "            continue\n",
    "        if use_null:\n",
    "            src_index -= 1\n",
    "            trg_index -= 1\n",
    "        if trg_index not in alignments:\n",
    "            alignments[trg_index] = {}\n",
    "        alignments[trg_index][src_index] = '*'\n",
    "    return alignments\n",
    "\n",
    "def align_corpus(src_corpus, trg_corpus, prior_model, translation_model, hmm=False,\n",
    "                 use_null=False, src_null_index=0, trg_null_index=0):\n",
    "    \"Align each sentence pair in the corpus in turn.\"\n",
    "    aligned_corpus = []\n",
    "    for src_tokens, trg_tokens in zip(src_corpus, trg_corpus):\n",
    "        if hmm:\n",
    "            posteriors, _, _, = infer_posteriors(src_tokens, trg_tokens, prior_model,\n",
    "                                                            translation_model, hmm=hmm)\n",
    "        else:\n",
    "            posteriors, _ = infer_posteriors(src_tokens, trg_tokens, prior_model,\n",
    "                                             translation_model, hmm=hmm)\n",
    "        alignments = get_alignments_from_posterior(posteriors, hmm=hmm, use_null=use_null,\n",
    "                                                   src_null_index=src_null_index,\n",
    "                                                   trg_null_index=trg_null_index)\n",
    "        aligned_corpus.append((src_tokens, trg_tokens, alignments))\n",
    "    return aligned_corpus\n",
    "\n",
    "def initialize_models(src_corpus, trg_corpus, identity_matrix, translation_model_cls,\n",
    "                      prior_model_cls, translation_model_=None, prior_model_=None,\n",
    "                      hard_align=False, **prior_params):\n",
    "    prior_model = (prior_model_cls(src_corpus, trg_corpus, **prior_params)\n",
    "                   if prior_model_ is None else prior_model_)\n",
    "    translation_model = (translation_model_cls(src_corpus, trg_corpus, identity_matrix, hard_align)\n",
    "                         if translation_model_ is None else translation_model_)\n",
    "    return prior_model, translation_model\n",
    "\n",
    "def load_lemmas(filenames):\n",
    "    word_to_lemma = {}\n",
    "    for filename in filenames:\n",
    "        with open(filename) as fin:\n",
    "            for line in fin:\n",
    "                lemma, word = line.strip().split()\n",
    "                word_to_lemma[word] = lemma\n",
    "    return word_to_lemma\n",
    "            \n",
    "            \n",
    "def normalize_corpus(corpus, use_null=False, null_token=\"<null>\",\n",
    "                     use_lemmas=False, lemmas_files=[], use_hashing=False, num_buckets=3000):\n",
    "    if use_lemmas:\n",
    "        word_to_lemma = load_lemmas(lemmas_files)\n",
    "        corpus = [list(map(lambda word: word_to_lemma.get(word.lower(), word.lower()), tokens))\n",
    "                  for tokens in corpus]\n",
    "    unique_tokens = sorted(set(token for tokens in corpus for token in tokens))\n",
    "    if use_null:\n",
    "        unique_tokens = [null_token] + unique_tokens\n",
    "    token_to_idx = {token: idx for idx, token in enumerate(unique_tokens)}\n",
    "    null_index = token_to_idx.get(null_token, None)\n",
    "    normalized_corpus = []\n",
    "    for tokens in corpus:\n",
    "        token_indices = [token_to_idx[token] for token in tokens]\n",
    "        if use_hashing:\n",
    "            offset = 1 if use_null else 0\n",
    "            token_indices = [offset + (hash(token) % num_buckets) for token in tokens]\n",
    "        else:\n",
    "            token_indices = [token_to_idx[token] for token in tokens]\n",
    "        if use_null:\n",
    "            token_indices = [null_index] + token_indices\n",
    "            \n",
    "        normalized_corpus.append(token_indices)\n",
    "    return normalized_corpus, unique_tokens, null_index\n",
    "\n",
    "def calc_trg_indices(src_data, unique_trg_tokens, use_editdistance,\n",
    "                     use_hashing, num_buckets, use_null):\n",
    "    trg_indices = []\n",
    "    src_idx, src_token = src_data\n",
    "    offset = 1 if use_null else 0\n",
    "    if use_hashing:\n",
    "        trg_tokens_with_indices = map(lambda token: (offset + (hash(token) % num_buckets), token),\n",
    "                                      unique_trg_tokens)\n",
    "    else:\n",
    "        trg_tokens_with_indices = enumerate(unique_trg_tokens)\n",
    "    for trg_idx, trg_token in trg_tokens_with_indices:\n",
    "        if (src_token == trg_token or\n",
    "            (use_editdistance and\n",
    "             (editdistance.eval(src_token, trg_token) / len(src_token)) < 0.2)):\n",
    "            trg_indices.append(trg_idx)\n",
    "    return trg_indices, src_idx, src_token\n",
    "\n",
    "def calc_identity_matrix(unique_src_tokens, unique_trg_tokens, use_editdistance,\n",
    "                         use_hashing, num_buckets, use_null):\n",
    "    iis = []\n",
    "    js = []\n",
    "    values = []\n",
    "    offset = 1 if use_null else 0\n",
    "    with multiprocessing.Pool(8) as pool:\n",
    "        map_func = functools.partial(calc_trg_indices,\n",
    "                                     unique_trg_tokens=unique_trg_tokens,\n",
    "                                     use_editdistance=use_editdistance,\n",
    "                                     use_hashing=use_hashing,\n",
    "                                     num_buckets=num_buckets,\n",
    "                                     use_null=use_null)\n",
    "        if use_hashing:\n",
    "            src_tokens_with_indices = map(lambda token: (offset + (hash(token) % num_buckets), token),\n",
    "                                          unique_src_tokens)\n",
    "        else:\n",
    "            src_tokens_with_indices = enumerate(unique_src_tokens)\n",
    "        for trg_indices, src_idx, src_token in pool.imap(map_func, src_tokens_with_indices):\n",
    "            iis.extend([src_idx] * len(trg_indices))\n",
    "            js.extend(trg_indices)\n",
    "            values.extend([1.0] * len(trg_indices))\n",
    "    if use_hashing:\n",
    "        shape = (offset + num_buckets, offset + num_buckets)\n",
    "    else:\n",
    "        shape = (len(unique_src_tokens), len(unique_trg_tokens))\n",
    "    return coo_matrix((values, (iis, js)), shape=shape)\n",
    "\n",
    "            \n",
    "\n",
    "def normalize(src_corpus, trg_corpus, use_null=False,\n",
    "              src_null_token=\"<src_null>\", trg_null_token=\"<trg_null>\",\n",
    "              use_editdistance=False, use_lemmas=False, lemmas_folder=\"lemmatization-lists\",\n",
    "              use_hashing=False, num_buckets=3000):\n",
    "    # assert False, \"Apply some normalization here to reduce the numbers of parameters.\"\n",
    "    (normalized_src, \n",
    "     unique_src_tokens,\n",
    "     src_null_index) = normalize_corpus(src_corpus, use_null, src_null_token,\n",
    "                                        use_lemmas, [os.path.join(lemmas_folder,\n",
    "                                                                  \"lemmatization-en.txt\")],\n",
    "                                        use_hashing, num_buckets)\n",
    "    (normalized_trg,\n",
    "     unique_trg_tokens,\n",
    "     trg_null_index) = normalize_corpus(trg_corpus, use_null, trg_null_token,\n",
    "                                        use_lemmas, [os.path.join(lemmas_folder,\n",
    "                                                                  \"lemmatization-sl.txt\"),\n",
    "                                                     os.path.join(lemmas_folder,\n",
    "                                                                  \"lemmatization-sk.txt\"),\n",
    "                                                     os.path.join(lemmas_folder,\n",
    "                                                                  \"lemmatization-cs.txt\")],\n",
    "                                        use_hashing, num_buckets)\n",
    "    identity_matrix = calc_identity_matrix(unique_src_tokens, unique_trg_tokens,\n",
    "                                           use_editdistance, use_hashing, num_buckets, use_null)\n",
    "    return normalized_src, normalized_trg, identity_matrix, src_null_index, trg_null_index\n",
    "\n",
    "def train(num_iterations, translation_model_cls=TranslationModel, prior_model_cls=PriorModel,\n",
    "          translation_model=None, prior_model=None, hmm=False, hard_align=False,\n",
    "          src_null_token=\"<src_null>\", trg_null_token=\"<trg_null>\", use_editdistance=False,\n",
    "          use_lemmas=False, lemmas_folder=\"lemmatization-lists\",\n",
    "          use_hashing=False, num_buckets=3000, **prior_params):\n",
    "    src_corpus, trg_corpus, _ = read_parallel_corpus('en-cs.all')\n",
    "    use_null = prior_params.get(\"use_null\", False)\n",
    "    if translation_model is not None:\n",
    "        use_editdistance = False\n",
    "    (src_corpus, trg_corpus, identity_matrix,\n",
    "     src_null_index, trg_null_index) = normalize(src_corpus, trg_corpus,\n",
    "                                                 use_null, src_null_token, trg_null_token,\n",
    "                                                 use_editdistance, use_lemmas, lemmas_folder,\n",
    "                                                 use_hashing, num_buckets)\n",
    "    if use_null and not hmm and prior_model_cls != PriorModel:\n",
    "        prior_params[\"src_null_index\"] = src_null_index\n",
    "        prior_params[\"trg_null_index\"] = trg_null_index\n",
    "    if use_null and (hmm or prior_model_cls == PriorModel):\n",
    "        del prior_params[\"use_null\"]\n",
    "    prior_model, translation_model = initialize_models(src_corpus, trg_corpus, identity_matrix,\n",
    "                                                       translation_model_cls, prior_model_cls,\n",
    "                                                       translation_model, prior_model, hard_align,\n",
    "                                                       **prior_params)\n",
    "    prior_model, translation_model = estimate_models(src_corpus, trg_corpus, prior_model,\n",
    "                                                     translation_model, num_iterations,\n",
    "                                                     hmm=hmm, use_null=use_null,\n",
    "                                                     src_null_index=src_null_index,\n",
    "                                                     trg_null_index=trg_null_index)    \n",
    "    aligned_corpus = align_corpus(src_corpus, trg_corpus, prior_model, translation_model,\n",
    "                                  hmm=hmm, use_null=use_null,\n",
    "                                  src_null_index=src_null_index, trg_null_index=trg_null_index)\n",
    "    return extract_test_set_alignments(aligned_corpus), translation_model, prior_model\n",
    "\n",
    "def evaluate(candidate_alignments):\n",
    "    src_dev, trg_dev, wa_dev = read_parallel_corpus('en-cs-wa.dev', has_alignments=True)\n",
    "    src_test, trg_test, wa_test = read_parallel_corpus('en-cs-wa.test', has_alignments=True)\n",
    "    print('dev: recall %1.3f; precision %1.3f; aer %1.3f' % score_alignments(wa_dev, candidate_alignments['dev']))\n",
    "    print('test: recall %1.3f; precision %1.3f; aer %1.3f' % score_alignments(wa_test, candidate_alignments['test']))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a simple IBM Model 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus log likelihood: -1389685.929\n",
      "dev: recall 0.424; precision 0.384; aer 0.598\n",
      "test: recall 0.424; precision 0.379; aer 0.601\n",
      "corpus log likelihood: -1238510.650\n",
      "dev: recall 0.465; precision 0.419; aer 0.560\n",
      "test: recall 0.464; precision 0.413; aer 0.564\n",
      "corpus log likelihood: -1173020.847\n",
      "dev: recall 0.480; precision 0.431; aer 0.547\n",
      "test: recall 0.477; precision 0.423; aer 0.553\n",
      "corpus log likelihood: -1147238.949\n",
      "dev: recall 0.487; precision 0.437; aer 0.540\n",
      "test: recall 0.485; precision 0.429; aer 0.546\n",
      "dev: recall 0.487; precision 0.437; aer 0.540\n",
      "test: recall 0.485; precision 0.429; aer 0.546\n"
     ]
    }
   ],
   "source": [
    "test_alignments, _, _ = train(5)\n",
    "evaluate(test_alignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will add hard-alignment explicitly setting alignment probability of identical tokens to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus log likelihood: -1340071.228\n",
      "dev: recall 0.440; precision 0.396; aer 0.584\n",
      "test: recall 0.439; precision 0.391; aer 0.587\n",
      "corpus log likelihood: -1215103.192\n",
      "dev: recall 0.478; precision 0.429; aer 0.549\n",
      "test: recall 0.476; precision 0.423; aer 0.553\n",
      "corpus log likelihood: -1158635.449\n",
      "dev: recall 0.491; precision 0.441; aer 0.536\n",
      "test: recall 0.488; precision 0.433; aer 0.542\n",
      "corpus log likelihood: -1136220.022\n",
      "dev: recall 0.497; precision 0.447; aer 0.530\n",
      "test: recall 0.495; precision 0.440; aer 0.535\n",
      "dev: recall 0.497; precision 0.447; aer 0.530\n",
      "test: recall 0.495; precision 0.440; aer 0.535\n"
     ]
    }
   ],
   "source": [
    "test_alignments, _, _ = train(5, hard_align=True)\n",
    "evaluate(test_alignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard alignment lowered our AER by 0.01. We will use it in all later experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try IBM Model 2 with a prior that depends on word positions in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/penguin138/.virtualenvs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:116: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus log likelihood: -1340071.228\n",
      "dev: recall 0.447; precision 0.404; aer 0.577\n",
      "test: recall 0.455; precision 0.405; aer 0.572\n",
      "corpus log likelihood: -1045077.915\n",
      "dev: recall 0.505; precision 0.456; aer 0.522\n",
      "test: recall 0.509; precision 0.454; aer 0.521\n",
      "corpus log likelihood: -853536.594\n",
      "dev: recall 0.525; precision 0.475; aer 0.502\n",
      "test: recall 0.528; precision 0.471; aer 0.503\n",
      "corpus log likelihood: -759527.242\n",
      "dev: recall 0.532; precision 0.482; aer 0.495\n",
      "test: recall 0.534; precision 0.478; aer 0.497\n",
      "dev: recall 0.532; precision 0.482; aer 0.495\n",
      "test: recall 0.534; precision 0.478; aer 0.497\n"
     ]
    }
   ],
   "source": [
    "test_alignments, _, _ = train(5, prior_model_cls=ComplexPriorModel, hard_align=True)\n",
    "evaluate(test_alignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More complex prior certainly improved our AER."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we pretrain translation model with IBM Model 1 for 2 epochs and then\n",
    "train IBM Model 2 using pretrained translation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus log likelihood: -1340071.228\n",
      "dev: recall 0.440; precision 0.396; aer 0.584\n",
      "test: recall 0.439; precision 0.391; aer 0.587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/penguin138/.virtualenvs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:116: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus log likelihood: -926555.541\n",
      "dev: recall 0.538; precision 0.486; aer 0.491\n",
      "test: recall 0.541; precision 0.483; aer 0.491\n",
      "corpus log likelihood: -790209.314\n",
      "dev: recall 0.546; precision 0.493; aer 0.483\n",
      "test: recall 0.548; precision 0.489; aer 0.484\n",
      "corpus log likelihood: -730269.906\n",
      "dev: recall 0.548; precision 0.496; aer 0.480\n",
      "test: recall 0.548; precision 0.490; aer 0.484\n",
      "corpus log likelihood: -701341.783\n",
      "dev: recall 0.548; precision 0.496; aer 0.480\n",
      "test: recall 0.550; precision 0.491; aer 0.482\n",
      "dev: recall 0.548; precision 0.496; aer 0.480\n",
      "test: recall 0.550; precision 0.491; aer 0.482\n"
     ]
    }
   ],
   "source": [
    "_, translation_model1, _ = train(2, hard_align=True)\n",
    "test_alignments, _, _  = train(5, prior_model_cls=ComplexPriorModel,\n",
    "                               translation_model=translation_model1)\n",
    "evaluate(test_alignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretrained model produces better AER than a model without pretraining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now our ComplexPrior depended on sentence lengths. We remove that dependency in ImprovedComplexPrior by using only relative position of the word in a sentence: $relative\\_pos = \\frac{word\\_index}{sentence\\_length}$. To simplify things, we introduce buckets, each of which will be responsible for one area of a sentence then we will use mapping from a relative position in a sentence to a bucket.\n",
    "We can calculate bucket numbers from relative positions as follows: $$bucket\\_number = \\lfloor{relative\\_pos \\cdot num\\_buckets}\\rfloor$$\n",
    "\n",
    "For example in a sentence \"Quick brown | fox jumps | over the | lazy dog\" where bucket borders are depicted using '|' token, the word 'jumps' has index 3 and therefore its relative position is $\\frac{3}{8} = 0.375$ and its bucket number is $\\lfloor0.375 * 4\\rfloor = 1$\n",
    "\n",
    "In our improved prior we use bucket indices instead of word indices, that way we reduce the number of parameters in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus log likelihood: -1059004.252\n",
      "dev: recall 0.519; precision 0.473; aer 0.506\n",
      "test: recall 0.526; precision 0.473; aer 0.503\n",
      "corpus log likelihood: -881497.886\n",
      "dev: recall 0.581; precision 0.529; aer 0.447\n",
      "test: recall 0.586; precision 0.528; aer 0.446\n",
      "corpus log likelihood: -763555.219\n",
      "dev: recall 0.593; precision 0.542; aer 0.435\n",
      "test: recall 0.599; precision 0.542; aer 0.432\n",
      "corpus log likelihood: -702661.794\n",
      "dev: recall 0.592; precision 0.542; aer 0.435\n",
      "test: recall 0.598; precision 0.541; aer 0.433\n",
      "dev: recall 0.592; precision 0.542; aer 0.435\n",
      "test: recall 0.598; precision 0.541; aer 0.433\n"
     ]
    }
   ],
   "source": [
    "test_alignments, _, _  = train(5, prior_model_cls=ImprovedComplexPriorModel,\n",
    "                               hard_align=True)\n",
    "evaluate(test_alignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to pretrain IBM Model 2 with improved prior using Model 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus log likelihood: -1340071.228\n",
      "dev: recall 0.440; precision 0.396; aer 0.584\n",
      "test: recall 0.439; precision 0.391; aer 0.587\n",
      "corpus log likelihood: -810766.506\n",
      "dev: recall 0.599; precision 0.545; aer 0.430\n",
      "test: recall 0.606; precision 0.546; aer 0.427\n",
      "corpus log likelihood: -734935.129\n",
      "dev: recall 0.602; precision 0.550; aer 0.426\n",
      "test: recall 0.607; precision 0.548; aer 0.425\n",
      "corpus log likelihood: -691945.668\n",
      "dev: recall 0.600; precision 0.549; aer 0.428\n",
      "test: recall 0.607; precision 0.549; aer 0.425\n",
      "corpus log likelihood: -668485.180\n",
      "dev: recall 0.598; precision 0.548; aer 0.429\n",
      "test: recall 0.604; precision 0.548; aer 0.427\n",
      "dev: recall 0.598; precision 0.548; aer 0.429\n",
      "test: recall 0.604; precision 0.548; aer 0.427\n"
     ]
    }
   ],
   "source": [
    "_, translation_model1, _ = train(2, hard_align=True)\n",
    "test_alignments, _, _  = train(5, prior_model_cls=ImprovedComplexPriorModel,\n",
    "                               translation_model=translation_model1)\n",
    "evaluate(test_alignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use HMM to train our alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus log likelihood: -1340071.228\n",
      "dev: recall 0.405; precision 0.368; aer 0.615\n",
      "test: recall 0.407; precision 0.364; aer 0.616\n",
      "corpus log likelihood: -1164938.254\n",
      "dev: recall 0.437; precision 0.399; aer 0.584\n",
      "test: recall 0.442; precision 0.398; aer 0.582\n",
      "corpus log likelihood: -1039038.072\n",
      "dev: recall 0.447; precision 0.412; aer 0.572\n",
      "test: recall 0.455; precision 0.413; aer 0.568\n",
      "corpus log likelihood: -940864.392\n",
      "dev: recall 0.448; precision 0.414; aer 0.571\n",
      "test: recall 0.456; precision 0.416; aer 0.566\n",
      "corpus log likelihood: -873764.473\n",
      "dev: recall 0.448; precision 0.414; aer 0.570\n",
      "test: recall 0.458; precision 0.417; aer 0.564\n",
      "corpus log likelihood: -837671.646\n",
      "dev: recall 0.444; precision 0.411; aer 0.574\n",
      "test: recall 0.457; precision 0.416; aer 0.566\n",
      "corpus log likelihood: -819925.310\n",
      "dev: recall 0.442; precision 0.409; aer 0.576\n",
      "test: recall 0.454; precision 0.414; aer 0.568\n",
      "corpus log likelihood: -810752.141\n",
      "dev: recall 0.442; precision 0.409; aer 0.576\n",
      "test: recall 0.454; precision 0.414; aer 0.568\n",
      "corpus log likelihood: -805564.318\n",
      "dev: recall 0.441; precision 0.408; aer 0.577\n",
      "test: recall 0.453; precision 0.414; aer 0.568\n",
      "dev: recall 0.441; precision 0.408; aer 0.577\n",
      "test: recall 0.453; precision 0.414; aer 0.568\n"
     ]
    }
   ],
   "source": [
    "test_alignments, _, _  = train(10, prior_model_cls=TransitionModel, hmm=True,\n",
    "                               hard_align=True)\n",
    "evaluate(test_alignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, HMM starts to diverge after 5-6 iterations, so it's no use to train it longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to add pretraining with Model 1 to HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus log likelihood: -1340071.228\n",
      "dev: recall 0.440; precision 0.396; aer 0.584\n",
      "test: recall 0.439; precision 0.391; aer 0.587\n",
      "corpus log likelihood: -1094438.564\n",
      "dev: recall 0.534; precision 0.486; aer 0.492\n",
      "test: recall 0.539; precision 0.483; aer 0.492\n",
      "corpus log likelihood: -991391.332\n",
      "dev: recall 0.536; precision 0.492; aer 0.488\n",
      "test: recall 0.542; precision 0.490; aer 0.486\n",
      "corpus log likelihood: -908109.303\n",
      "dev: recall 0.536; precision 0.493; aer 0.488\n",
      "test: recall 0.542; precision 0.492; aer 0.485\n",
      "corpus log likelihood: -855545.814\n",
      "dev: recall 0.534; precision 0.492; aer 0.489\n",
      "test: recall 0.540; precision 0.490; aer 0.487\n",
      "corpus log likelihood: -827657.895\n",
      "dev: recall 0.532; precision 0.491; aer 0.491\n",
      "test: recall 0.538; precision 0.489; aer 0.489\n",
      "dev: recall 0.532; precision 0.491; aer 0.491\n",
      "test: recall 0.538; precision 0.489; aer 0.489\n"
     ]
    }
   ],
   "source": [
    "_, translation_model1, _ = train(2, hard_align=True)\n",
    "test_alignments, _, _  = train(6, prior_model_cls=TransitionModel,\n",
    "                               translation_model=translation_model1,\n",
    "                               hmm=True)\n",
    "evaluate(test_alignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to optimise parameters of Model 2 with Model 1 pretraining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus log likelihood: -1340071.228\n",
      "dev: recall 0.440; precision 0.396; aer 0.584\n",
      "test: recall 0.439; precision 0.391; aer 0.587\n",
      "corpus log likelihood: -1215103.192\n",
      "dev: recall 0.478; precision 0.429; aer 0.549\n",
      "test: recall 0.476; precision 0.423; aer 0.553\n",
      "corpus log likelihood: -1158635.449\n",
      "dev: recall 0.491; precision 0.441; aer 0.536\n",
      "test: recall 0.488; precision 0.433; aer 0.542\n",
      "corpus log likelihood: -1136220.022\n",
      "dev: recall 0.497; precision 0.447; aer 0.530\n",
      "test: recall 0.495; precision 0.440; aer 0.535\n",
      "corpus log likelihood: -1126356.169\n",
      "dev: recall 0.500; precision 0.448; aer 0.528\n",
      "test: recall 0.498; precision 0.441; aer 0.533\n",
      "corpus log likelihood: -1121308.055\n",
      "dev: recall 0.502; precision 0.449; aer 0.527\n",
      "test: recall 0.500; precision 0.443; aer 0.531\n",
      "corpus log likelihood: -1118403.061\n",
      "dev: recall 0.504; precision 0.451; aer 0.525\n",
      "test: recall 0.502; precision 0.444; aer 0.529\n",
      "corpus log likelihood: -1116584.283\n",
      "dev: recall 0.505; precision 0.452; aer 0.524\n",
      "test: recall 0.502; precision 0.445; aer 0.529\n",
      "corpus log likelihood: -1115372.438\n",
      "dev: recall 0.507; precision 0.453; aer 0.522\n",
      "test: recall 0.504; precision 0.446; aer 0.528\n",
      "corpus log likelihood: -749697.264\n",
      "dev: recall 0.602; precision 0.545; aer 0.429\n",
      "test: recall 0.608; precision 0.545; aer 0.427\n",
      "corpus log likelihood: -723315.584\n",
      "dev: recall 0.610; precision 0.554; aer 0.421\n",
      "test: recall 0.614; precision 0.551; aer 0.421\n",
      "corpus log likelihood: -704156.904\n",
      "dev: recall 0.611; precision 0.555; aer 0.419\n",
      "test: recall 0.615; precision 0.553; aer 0.419\n",
      "corpus log likelihood: -689305.907\n",
      "dev: recall 0.610; precision 0.556; aer 0.419\n",
      "test: recall 0.616; precision 0.554; aer 0.418\n",
      "corpus log likelihood: -678469.732\n",
      "dev: recall 0.609; precision 0.556; aer 0.420\n",
      "test: recall 0.615; precision 0.553; aer 0.419\n",
      "corpus log likelihood: -670506.363\n",
      "dev: recall 0.607; precision 0.555; aer 0.421\n",
      "test: recall 0.614; precision 0.553; aer 0.419\n",
      "corpus log likelihood: -664478.619\n",
      "dev: recall 0.605; precision 0.553; aer 0.423\n",
      "test: recall 0.613; precision 0.553; aer 0.420\n",
      "corpus log likelihood: -659717.921\n",
      "dev: recall 0.604; precision 0.553; aer 0.424\n",
      "test: recall 0.612; precision 0.553; aer 0.420\n",
      "corpus log likelihood: -655885.084\n",
      "dev: recall 0.602; precision 0.552; aer 0.425\n",
      "test: recall 0.611; precision 0.553; aer 0.421\n",
      "corpus log likelihood: -652709.756\n",
      "dev: recall 0.603; precision 0.552; aer 0.425\n",
      "test: recall 0.608; precision 0.551; aer 0.423\n",
      "corpus log likelihood: -650031.120\n",
      "dev: recall 0.601; precision 0.552; aer 0.426\n",
      "test: recall 0.606; precision 0.549; aer 0.425\n",
      "corpus log likelihood: -647754.111\n",
      "dev: recall 0.599; precision 0.551; aer 0.427\n",
      "test: recall 0.605; precision 0.548; aer 0.426\n",
      "corpus log likelihood: -645787.112\n",
      "dev: recall 0.599; precision 0.550; aer 0.428\n",
      "test: recall 0.602; precision 0.546; aer 0.429\n",
      "corpus log likelihood: -644069.485\n",
      "dev: recall 0.598; precision 0.549; aer 0.429\n",
      "test: recall 0.601; precision 0.545; aer 0.430\n",
      "dev: recall 0.598; precision 0.549; aer 0.429\n",
      "test: recall 0.601; precision 0.545; aer 0.430\n"
     ]
    }
   ],
   "source": [
    "_, translation_model1, _ = train(10, hard_align=True)\n",
    "test_alignments, _, _  = train(15, prior_model_cls=ImprovedComplexPriorModel,\n",
    "                               translation_model=translation_model1)\n",
    "evaluate(test_alignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's increase the number of buckets (default was 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus log likelihood: -1340071.228\n",
      "dev: recall 0.440; precision 0.396; aer 0.584\n",
      "test: recall 0.439; precision 0.391; aer 0.587\n",
      "corpus log likelihood: -1215103.192\n",
      "dev: recall 0.478; precision 0.429; aer 0.549\n",
      "test: recall 0.476; precision 0.423; aer 0.553\n",
      "corpus log likelihood: -1158635.449\n",
      "dev: recall 0.491; precision 0.441; aer 0.536\n",
      "test: recall 0.488; precision 0.433; aer 0.542\n",
      "corpus log likelihood: -1136220.022\n",
      "dev: recall 0.497; precision 0.447; aer 0.530\n",
      "test: recall 0.495; precision 0.440; aer 0.535\n",
      "corpus log likelihood: -1126356.169\n",
      "dev: recall 0.500; precision 0.448; aer 0.528\n",
      "test: recall 0.498; precision 0.441; aer 0.533\n",
      "corpus log likelihood: -1121308.055\n",
      "dev: recall 0.502; precision 0.449; aer 0.527\n",
      "test: recall 0.500; precision 0.443; aer 0.531\n",
      "corpus log likelihood: -1118403.061\n",
      "dev: recall 0.504; precision 0.451; aer 0.525\n",
      "test: recall 0.502; precision 0.444; aer 0.529\n",
      "corpus log likelihood: -1116584.283\n",
      "dev: recall 0.505; precision 0.452; aer 0.524\n",
      "test: recall 0.502; precision 0.445; aer 0.529\n",
      "corpus log likelihood: -1115372.438\n",
      "dev: recall 0.507; precision 0.453; aer 0.522\n",
      "test: recall 0.504; precision 0.446; aer 0.528\n",
      "corpus log likelihood: -846987.302\n",
      "dev: recall 0.606; precision 0.550; aer 0.424\n",
      "test: recall 0.610; precision 0.546; aer 0.425\n",
      "corpus log likelihood: -816314.474\n",
      "dev: recall 0.614; precision 0.558; aer 0.417\n",
      "test: recall 0.618; precision 0.554; aer 0.417\n",
      "corpus log likelihood: -796996.288\n",
      "dev: recall 0.617; precision 0.561; aer 0.413\n",
      "test: recall 0.623; precision 0.559; aer 0.412\n",
      "corpus log likelihood: -782668.073\n",
      "dev: recall 0.620; precision 0.565; aer 0.410\n",
      "test: recall 0.625; precision 0.562; aer 0.409\n",
      "corpus log likelihood: -771886.364\n",
      "dev: recall 0.619; precision 0.565; aer 0.410\n",
      "test: recall 0.626; precision 0.564; aer 0.408\n",
      "corpus log likelihood: -763718.066\n",
      "dev: recall 0.619; precision 0.566; aer 0.410\n",
      "test: recall 0.626; precision 0.564; aer 0.408\n",
      "corpus log likelihood: -757392.818\n",
      "dev: recall 0.618; precision 0.565; aer 0.411\n",
      "test: recall 0.624; precision 0.564; aer 0.409\n",
      "corpus log likelihood: -752393.655\n",
      "dev: recall 0.616; precision 0.564; aer 0.413\n",
      "test: recall 0.623; precision 0.563; aer 0.410\n",
      "corpus log likelihood: -748318.962\n",
      "dev: recall 0.613; precision 0.563; aer 0.414\n",
      "test: recall 0.622; precision 0.563; aer 0.410\n",
      "corpus log likelihood: -744853.698\n",
      "dev: recall 0.611; precision 0.561; aer 0.416\n",
      "test: recall 0.619; precision 0.561; aer 0.413\n",
      "corpus log likelihood: -741894.313\n",
      "dev: recall 0.610; precision 0.560; aer 0.417\n",
      "test: recall 0.618; precision 0.560; aer 0.414\n",
      "corpus log likelihood: -739437.118\n",
      "dev: recall 0.608; precision 0.560; aer 0.418\n",
      "test: recall 0.616; precision 0.558; aer 0.415\n",
      "corpus log likelihood: -737359.505\n",
      "dev: recall 0.607; precision 0.558; aer 0.420\n",
      "test: recall 0.615; precision 0.557; aer 0.417\n",
      "corpus log likelihood: -735531.693\n",
      "dev: recall 0.605; precision 0.556; aer 0.422\n",
      "test: recall 0.614; precision 0.557; aer 0.418\n",
      "dev: recall 0.605; precision 0.556; aer 0.422\n",
      "test: recall 0.614; precision 0.557; aer 0.418\n"
     ]
    }
   ],
   "source": [
    "_, translation_model1, _ = train(10, hard_align=True)\n",
    "test_alignments, _, _  = train(15, prior_model_cls=ImprovedComplexPriorModel,\n",
    "                               translation_model=translation_model1,\n",
    "                               num_indices=15)\n",
    "evaluate(test_alignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus log likelihood: -1340071.228\n",
      "dev: recall 0.440; precision 0.396; aer 0.584\n",
      "test: recall 0.439; precision 0.391; aer 0.587\n",
      "corpus log likelihood: -1215103.192\n",
      "dev: recall 0.478; precision 0.429; aer 0.549\n",
      "test: recall 0.476; precision 0.423; aer 0.553\n",
      "corpus log likelihood: -1158635.449\n",
      "dev: recall 0.491; precision 0.441; aer 0.536\n",
      "test: recall 0.488; precision 0.433; aer 0.542\n",
      "corpus log likelihood: -1136220.022\n",
      "dev: recall 0.497; precision 0.447; aer 0.530\n",
      "test: recall 0.495; precision 0.440; aer 0.535\n",
      "corpus log likelihood: -1126356.169\n",
      "dev: recall 0.500; precision 0.448; aer 0.528\n",
      "test: recall 0.498; precision 0.441; aer 0.533\n",
      "corpus log likelihood: -1121308.055\n",
      "dev: recall 0.502; precision 0.449; aer 0.527\n",
      "test: recall 0.500; precision 0.443; aer 0.531\n",
      "corpus log likelihood: -1118403.061\n",
      "dev: recall 0.504; precision 0.451; aer 0.525\n",
      "test: recall 0.502; precision 0.444; aer 0.529\n",
      "corpus log likelihood: -1116584.283\n",
      "dev: recall 0.505; precision 0.452; aer 0.524\n",
      "test: recall 0.502; precision 0.445; aer 0.529\n",
      "corpus log likelihood: -1115372.438\n",
      "dev: recall 0.507; precision 0.453; aer 0.522\n",
      "test: recall 0.504; precision 0.446; aer 0.528\n",
      "corpus log likelihood: -916341.526\n",
      "dev: recall 0.611; precision 0.554; aer 0.420\n",
      "test: recall 0.616; precision 0.552; aer 0.419\n",
      "corpus log likelihood: -883202.889\n",
      "dev: recall 0.618; precision 0.562; aer 0.413\n",
      "test: recall 0.624; precision 0.560; aer 0.411\n",
      "corpus log likelihood: -862823.526\n",
      "dev: recall 0.623; precision 0.566; aer 0.408\n",
      "test: recall 0.629; precision 0.565; aer 0.406\n",
      "corpus log likelihood: -848181.613\n",
      "dev: recall 0.626; precision 0.570; aer 0.405\n",
      "test: recall 0.629; precision 0.566; aer 0.405\n",
      "corpus log likelihood: -837225.454\n",
      "dev: recall 0.626; precision 0.572; aer 0.403\n",
      "test: recall 0.630; precision 0.568; aer 0.404\n",
      "corpus log likelihood: -828793.595\n",
      "dev: recall 0.625; precision 0.572; aer 0.404\n",
      "test: recall 0.629; precision 0.568; aer 0.404\n",
      "corpus log likelihood: -822114.961\n",
      "dev: recall 0.624; precision 0.571; aer 0.405\n",
      "test: recall 0.629; precision 0.569; aer 0.404\n",
      "corpus log likelihood: -816787.565\n",
      "dev: recall 0.623; precision 0.570; aer 0.406\n",
      "test: recall 0.628; precision 0.568; aer 0.404\n",
      "corpus log likelihood: -812396.980\n",
      "dev: recall 0.621; precision 0.569; aer 0.407\n",
      "test: recall 0.627; precision 0.567; aer 0.406\n",
      "corpus log likelihood: -808711.938\n",
      "dev: recall 0.620; precision 0.569; aer 0.408\n",
      "test: recall 0.625; precision 0.565; aer 0.408\n",
      "corpus log likelihood: -805638.672\n",
      "dev: recall 0.618; precision 0.567; aer 0.409\n",
      "test: recall 0.623; precision 0.564; aer 0.409\n",
      "corpus log likelihood: -803021.061\n",
      "dev: recall 0.616; precision 0.566; aer 0.411\n",
      "test: recall 0.622; precision 0.563; aer 0.410\n",
      "corpus log likelihood: -800803.002\n",
      "dev: recall 0.614; precision 0.565; aer 0.413\n",
      "test: recall 0.620; precision 0.562; aer 0.412\n",
      "corpus log likelihood: -798867.097\n",
      "dev: recall 0.613; precision 0.563; aer 0.414\n",
      "test: recall 0.617; precision 0.560; aer 0.414\n",
      "dev: recall 0.613; precision 0.563; aer 0.414\n",
      "test: recall 0.617; precision 0.560; aer 0.414\n"
     ]
    }
   ],
   "source": [
    "_, translation_model1, _ = train(10, hard_align=True)\n",
    "test_alignments, _, _  = train(15, prior_model_cls=ImprovedComplexPriorModel,\n",
    "                               translation_model=translation_model1,\n",
    "                               num_indices=20)\n",
    "evaluate(test_alignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus log likelihood: -1340071.228\n",
      "dev: recall 0.440; precision 0.396; aer 0.584\n",
      "test: recall 0.439; precision 0.391; aer 0.587\n",
      "corpus log likelihood: -1215103.192\n",
      "dev: recall 0.478; precision 0.429; aer 0.549\n",
      "test: recall 0.476; precision 0.423; aer 0.553\n",
      "corpus log likelihood: -1158635.449\n",
      "dev: recall 0.491; precision 0.441; aer 0.536\n",
      "test: recall 0.488; precision 0.433; aer 0.542\n",
      "corpus log likelihood: -1136220.022\n",
      "dev: recall 0.497; precision 0.447; aer 0.530\n",
      "test: recall 0.495; precision 0.440; aer 0.535\n",
      "corpus log likelihood: -1126356.169\n",
      "dev: recall 0.500; precision 0.448; aer 0.528\n",
      "test: recall 0.498; precision 0.441; aer 0.533\n",
      "corpus log likelihood: -1121308.055\n",
      "dev: recall 0.502; precision 0.449; aer 0.527\n",
      "test: recall 0.500; precision 0.443; aer 0.531\n",
      "corpus log likelihood: -1118403.061\n",
      "dev: recall 0.504; precision 0.451; aer 0.525\n",
      "test: recall 0.502; precision 0.444; aer 0.529\n",
      "corpus log likelihood: -1116584.283\n",
      "dev: recall 0.505; precision 0.452; aer 0.524\n",
      "test: recall 0.502; precision 0.445; aer 0.529\n",
      "corpus log likelihood: -1115372.438\n",
      "dev: recall 0.507; precision 0.453; aer 0.522\n",
      "test: recall 0.504; precision 0.446; aer 0.528\n",
      "corpus log likelihood: -972239.954\n",
      "dev: recall 0.611; precision 0.554; aer 0.420\n",
      "test: recall 0.616; precision 0.552; aer 0.419\n",
      "corpus log likelihood: -937968.282\n",
      "dev: recall 0.619; precision 0.563; aer 0.412\n",
      "test: recall 0.624; precision 0.560; aer 0.411\n",
      "corpus log likelihood: -916977.742\n",
      "dev: recall 0.625; precision 0.568; aer 0.406\n",
      "test: recall 0.628; precision 0.563; aer 0.407\n",
      "corpus log likelihood: -902214.864\n",
      "dev: recall 0.626; precision 0.570; aer 0.404\n",
      "test: recall 0.630; precision 0.566; aer 0.405\n",
      "corpus log likelihood: -891142.685\n",
      "dev: recall 0.626; precision 0.571; aer 0.404\n",
      "test: recall 0.630; precision 0.568; aer 0.404\n",
      "corpus log likelihood: -882651.372\n",
      "dev: recall 0.624; precision 0.571; aer 0.405\n",
      "test: recall 0.632; precision 0.570; aer 0.402\n",
      "corpus log likelihood: -875960.624\n",
      "dev: recall 0.622; precision 0.570; aer 0.406\n",
      "test: recall 0.630; precision 0.569; aer 0.404\n",
      "corpus log likelihood: -870572.218\n",
      "dev: recall 0.620; precision 0.568; aer 0.409\n",
      "test: recall 0.629; precision 0.568; aer 0.404\n",
      "corpus log likelihood: -866177.590\n",
      "dev: recall 0.618; precision 0.567; aer 0.410\n",
      "test: recall 0.628; precision 0.568; aer 0.405\n",
      "corpus log likelihood: -862467.178\n",
      "dev: recall 0.617; precision 0.567; aer 0.410\n",
      "test: recall 0.627; precision 0.567; aer 0.406\n",
      "corpus log likelihood: -859370.432\n",
      "dev: recall 0.616; precision 0.566; aer 0.411\n",
      "test: recall 0.625; precision 0.566; aer 0.407\n",
      "corpus log likelihood: -856703.610\n",
      "dev: recall 0.614; precision 0.565; aer 0.413\n",
      "test: recall 0.623; precision 0.565; aer 0.409\n",
      "corpus log likelihood: -854424.866\n",
      "dev: recall 0.613; precision 0.564; aer 0.414\n",
      "test: recall 0.622; precision 0.564; aer 0.410\n",
      "corpus log likelihood: -852467.235\n",
      "dev: recall 0.612; precision 0.564; aer 0.415\n",
      "test: recall 0.620; precision 0.563; aer 0.411\n",
      "dev: recall 0.612; precision 0.564; aer 0.415\n",
      "test: recall 0.620; precision 0.563; aer 0.411\n"
     ]
    }
   ],
   "source": [
    "_, translation_model1, _ = train(10, hard_align=True)\n",
    "test_alignments, _, _  = train(15, prior_model_cls=ImprovedComplexPriorModel,\n",
    "                               translation_model=translation_model1,\n",
    "                               num_indices=25)\n",
    "evaluate(test_alignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All versions of Model 2 with pretraining start to diverge after 5-6 iterations, so there is no point in training it further. Model with num_buckets=20 gives the best AER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the best chained pretraining model to pretrain HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus log likelihood: -1340071.228\n",
      "dev: recall 0.440; precision 0.396; aer 0.584\n",
      "test: recall 0.439; precision 0.391; aer 0.587\n",
      "corpus log likelihood: -1215103.192\n",
      "dev: recall 0.478; precision 0.429; aer 0.549\n",
      "test: recall 0.476; precision 0.423; aer 0.553\n",
      "corpus log likelihood: -1158635.449\n",
      "dev: recall 0.491; precision 0.441; aer 0.536\n",
      "test: recall 0.488; precision 0.433; aer 0.542\n",
      "corpus log likelihood: -1136220.022\n",
      "dev: recall 0.497; precision 0.447; aer 0.530\n",
      "test: recall 0.495; precision 0.440; aer 0.535\n",
      "corpus log likelihood: -1126356.169\n",
      "dev: recall 0.500; precision 0.448; aer 0.528\n",
      "test: recall 0.498; precision 0.441; aer 0.533\n",
      "corpus log likelihood: -1121308.055\n",
      "dev: recall 0.502; precision 0.449; aer 0.527\n",
      "test: recall 0.500; precision 0.443; aer 0.531\n",
      "corpus log likelihood: -1118403.061\n",
      "dev: recall 0.504; precision 0.451; aer 0.525\n",
      "test: recall 0.502; precision 0.444; aer 0.529\n",
      "corpus log likelihood: -1116584.283\n",
      "dev: recall 0.505; precision 0.452; aer 0.524\n",
      "test: recall 0.502; precision 0.445; aer 0.529\n",
      "corpus log likelihood: -1115372.438\n",
      "dev: recall 0.507; precision 0.453; aer 0.522\n",
      "test: recall 0.504; precision 0.446; aer 0.528\n",
      "corpus log likelihood: -916341.526\n",
      "dev: recall 0.611; precision 0.554; aer 0.420\n",
      "test: recall 0.616; precision 0.552; aer 0.419\n",
      "corpus log likelihood: -883202.889\n",
      "dev: recall 0.618; precision 0.562; aer 0.413\n",
      "test: recall 0.624; precision 0.560; aer 0.411\n",
      "corpus log likelihood: -862823.526\n",
      "dev: recall 0.623; precision 0.566; aer 0.408\n",
      "test: recall 0.629; precision 0.565; aer 0.406\n",
      "corpus log likelihood: -848181.613\n",
      "dev: recall 0.626; precision 0.570; aer 0.405\n",
      "test: recall 0.629; precision 0.566; aer 0.405\n",
      "corpus log likelihood: -837225.454\n",
      "dev: recall 0.626; precision 0.572; aer 0.403\n",
      "test: recall 0.630; precision 0.568; aer 0.404\n",
      "corpus log likelihood: -953804.727\n",
      "dev: recall 0.631; precision 0.576; aer 0.399\n",
      "test: recall 0.641; precision 0.577; aer 0.394\n",
      "corpus log likelihood: -896103.418\n",
      "dev: recall 0.631; precision 0.579; aer 0.397\n",
      "test: recall 0.643; precision 0.579; aer 0.392\n",
      "corpus log likelihood: -868336.418\n",
      "dev: recall 0.631; precision 0.580; aer 0.397\n",
      "test: recall 0.642; precision 0.580; aer 0.392\n",
      "corpus log likelihood: -852356.291\n",
      "dev: recall 0.631; precision 0.580; aer 0.397\n",
      "test: recall 0.641; precision 0.579; aer 0.393\n",
      "dev: recall 0.631; precision 0.580; aer 0.397\n",
      "test: recall 0.641; precision 0.579; aer 0.393\n"
     ]
    }
   ],
   "source": [
    "_, translation_model1, _ = train(10, hard_align=True)\n",
    "_, translation_model2, _  = train(6, prior_model_cls=ImprovedComplexPriorModel,\n",
    "                                  translation_model=translation_model1,\n",
    "                                  num_indices=20)\n",
    "test_alignments, _, _ = train(5, prior_model_cls=TransitionModel,\n",
    "                              translation_model=translation_model2, hmm=True)\n",
    "evaluate(test_alignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding data normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've experimented with different models, we can improve them further by modifying our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NULL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with adding NULL tokens to source and target sentences, so that our models have the option of not aligning the word anywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus log likelihood: -1396500.374\n",
      "dev: recall 0.435; precision 0.393; aer 0.588\n",
      "test: recall 0.436; precision 0.388; aer 0.590\n",
      "corpus log likelihood: -1264417.109\n",
      "dev: recall 0.476; precision 0.428; aer 0.550\n",
      "test: recall 0.473; precision 0.422; aer 0.555\n",
      "corpus log likelihood: -1206642.928\n",
      "dev: recall 0.490; precision 0.440; aer 0.537\n",
      "test: recall 0.486; precision 0.433; aer 0.543\n",
      "corpus log likelihood: -1183555.582\n",
      "dev: recall 0.495; precision 0.445; aer 0.532\n",
      "test: recall 0.493; precision 0.439; aer 0.537\n",
      "dev: recall 0.495; precision 0.445; aer 0.532\n",
      "test: recall 0.493; precision 0.439; aer 0.537\n"
     ]
    }
   ],
   "source": [
    "test_alignments, _, _ = train(5, hard_align=True, use_null=True)\n",
    "evaluate(test_alignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/penguin138/.virtualenvs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:116: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus log likelihood: -1777093.628\n",
      "dev: recall 0.443; precision 0.401; aer 0.580\n",
      "test: recall 0.452; precision 0.402; aer 0.575\n",
      "corpus log likelihood: -1465544.316\n",
      "dev: recall 0.502; precision 0.454; aer 0.524\n",
      "test: recall 0.505; precision 0.451; aer 0.524\n",
      "corpus log likelihood: -1260819.822\n",
      "dev: recall 0.522; precision 0.473; aer 0.505\n",
      "test: recall 0.526; precision 0.470; aer 0.505\n",
      "corpus log likelihood: -1148278.615\n",
      "dev: recall 0.530; precision 0.480; aer 0.497\n",
      "test: recall 0.533; precision 0.477; aer 0.498\n",
      "dev: recall 0.530; precision 0.480; aer 0.497\n",
      "test: recall 0.533; precision 0.477; aer 0.498\n"
     ]
    }
   ],
   "source": [
    "test_alignments, _, _ = train(5, prior_model_cls=ComplexPriorModel, hard_align=True,\n",
    "                              use_null=True)\n",
    "evaluate(test_alignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus log likelihood: -1666450.611\n",
      "dev: recall 0.516; precision 0.469; aer 0.510\n",
      "test: recall 0.527; precision 0.473; aer 0.502\n",
      "corpus log likelihood: -1477549.541\n",
      "dev: recall 0.589; precision 0.537; aer 0.439\n",
      "test: recall 0.597; precision 0.537; aer 0.436\n",
      "corpus log likelihood: -1335835.561\n",
      "dev: recall 0.608; precision 0.557; aer 0.420\n",
      "test: recall 0.621; precision 0.561; aer 0.412\n",
      "corpus log likelihood: -1254862.775\n",
      "dev: recall 0.610; precision 0.560; aer 0.417\n",
      "test: recall 0.621; precision 0.564; aer 0.410\n",
      "dev: recall 0.610; precision 0.560; aer 0.417\n",
      "test: recall 0.621; precision 0.564; aer 0.410\n"
     ]
    }
   ],
   "source": [
    "test_alignments, _, _ = train(5, prior_model_cls=ImprovedComplexPriorModel, hard_align=True,\n",
    "                              use_null=True, num_indices=20)\n",
    "evaluate(test_alignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus log likelihood: -1396500.374\n",
      "dev: recall 0.435; precision 0.393; aer 0.588\n",
      "test: recall 0.436; precision 0.388; aer 0.590\n",
      "corpus log likelihood: -1264417.109\n",
      "dev: recall 0.476; precision 0.428; aer 0.550\n",
      "test: recall 0.473; precision 0.422; aer 0.555\n",
      "corpus log likelihood: -1206642.928\n",
      "dev: recall 0.490; precision 0.440; aer 0.537\n",
      "test: recall 0.486; precision 0.433; aer 0.543\n",
      "corpus log likelihood: -1183555.582\n",
      "dev: recall 0.495; precision 0.445; aer 0.532\n",
      "test: recall 0.493; precision 0.439; aer 0.537\n",
      "corpus log likelihood: -1173258.251\n",
      "dev: recall 0.500; precision 0.449; aer 0.528\n",
      "test: recall 0.498; precision 0.442; aer 0.533\n",
      "corpus log likelihood: -1167925.793\n",
      "dev: recall 0.501; precision 0.450; aer 0.527\n",
      "test: recall 0.499; precision 0.444; aer 0.531\n",
      "corpus log likelihood: -1164826.667\n",
      "dev: recall 0.503; precision 0.451; aer 0.525\n",
      "test: recall 0.501; precision 0.445; aer 0.530\n",
      "corpus log likelihood: -1162869.478\n",
      "dev: recall 0.504; precision 0.453; aer 0.524\n",
      "test: recall 0.502; precision 0.446; aer 0.529\n",
      "corpus log likelihood: -1161555.793\n",
      "dev: recall 0.505; precision 0.454; aer 0.523\n",
      "test: recall 0.503; precision 0.446; aer 0.528\n",
      "corpus log likelihood: -1310273.384\n",
      "dev: recall 0.609; precision 0.553; aer 0.421\n",
      "test: recall 0.617; precision 0.554; aer 0.418\n",
      "corpus log likelihood: -1268288.752\n",
      "dev: recall 0.620; precision 0.564; aer 0.411\n",
      "test: recall 0.625; precision 0.561; aer 0.410\n",
      "corpus log likelihood: -1245827.481\n",
      "dev: recall 0.622; precision 0.567; aer 0.408\n",
      "test: recall 0.628; precision 0.565; aer 0.407\n",
      "corpus log likelihood: -1230159.470\n",
      "dev: recall 0.623; precision 0.569; aer 0.406\n",
      "test: recall 0.632; precision 0.569; aer 0.402\n",
      "corpus log likelihood: -1218632.889\n",
      "dev: recall 0.624; precision 0.570; aer 0.405\n",
      "test: recall 0.632; precision 0.569; aer 0.403\n",
      "dev: recall 0.624; precision 0.570; aer 0.405\n",
      "test: recall 0.632; precision 0.569; aer 0.403\n"
     ]
    }
   ],
   "source": [
    "_, translation_model1, _ = train(10, hard_align=True, use_null=True)\n",
    "test_alignments, _, _  = train(6, prior_model_cls=ImprovedComplexPriorModel,\n",
    "                               translation_model=translation_model1,\n",
    "                               num_indices=20, use_null=True)\n",
    "evaluate(test_alignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus log likelihood: -1396500.374\n",
      "dev: recall 0.435; precision 0.393; aer 0.588\n",
      "test: recall 0.436; precision 0.388; aer 0.590\n",
      "corpus log likelihood: -1264417.109\n",
      "dev: recall 0.476; precision 0.428; aer 0.550\n",
      "test: recall 0.473; precision 0.422; aer 0.555\n",
      "corpus log likelihood: -1206642.928\n",
      "dev: recall 0.490; precision 0.440; aer 0.537\n",
      "test: recall 0.486; precision 0.433; aer 0.543\n",
      "corpus log likelihood: -1183555.582\n",
      "dev: recall 0.495; precision 0.445; aer 0.532\n",
      "test: recall 0.493; precision 0.439; aer 0.537\n",
      "corpus log likelihood: -1173258.251\n",
      "dev: recall 0.500; precision 0.449; aer 0.528\n",
      "test: recall 0.498; precision 0.442; aer 0.533\n",
      "corpus log likelihood: -1167925.793\n",
      "dev: recall 0.501; precision 0.450; aer 0.527\n",
      "test: recall 0.499; precision 0.444; aer 0.531\n",
      "corpus log likelihood: -1164826.667\n",
      "dev: recall 0.503; precision 0.451; aer 0.525\n",
      "test: recall 0.501; precision 0.445; aer 0.530\n",
      "corpus log likelihood: -1162869.478\n",
      "dev: recall 0.504; precision 0.453; aer 0.524\n",
      "test: recall 0.502; precision 0.446; aer 0.529\n",
      "corpus log likelihood: -1161555.793\n",
      "dev: recall 0.505; precision 0.454; aer 0.523\n",
      "test: recall 0.503; precision 0.446; aer 0.528\n",
      "corpus log likelihood: -1310273.384\n",
      "dev: recall 0.609; precision 0.553; aer 0.421\n",
      "test: recall 0.617; precision 0.554; aer 0.418\n",
      "corpus log likelihood: -1268288.752\n",
      "dev: recall 0.620; precision 0.564; aer 0.411\n",
      "test: recall 0.625; precision 0.561; aer 0.410\n",
      "corpus log likelihood: -1245827.481\n",
      "dev: recall 0.622; precision 0.567; aer 0.408\n",
      "test: recall 0.628; precision 0.565; aer 0.407\n",
      "corpus log likelihood: -1230159.470\n",
      "dev: recall 0.623; precision 0.569; aer 0.406\n",
      "test: recall 0.632; precision 0.569; aer 0.402\n",
      "corpus log likelihood: -1218632.889\n",
      "dev: recall 0.624; precision 0.570; aer 0.405\n",
      "test: recall 0.632; precision 0.569; aer 0.403\n",
      "corpus log likelihood: -981174.010\n",
      "dev: recall 0.635; precision 0.580; aer 0.395\n",
      "test: recall 0.645; precision 0.582; aer 0.390\n",
      "corpus log likelihood: -922144.635\n",
      "dev: recall 0.637; precision 0.584; aer 0.392\n",
      "test: recall 0.648; precision 0.585; aer 0.386\n",
      "corpus log likelihood: -893529.387\n",
      "dev: recall 0.637; precision 0.585; aer 0.391\n",
      "test: recall 0.649; precision 0.588; aer 0.384\n",
      "corpus log likelihood: -876883.507\n",
      "dev: recall 0.638; precision 0.587; aer 0.390\n",
      "test: recall 0.650; precision 0.589; aer 0.384\n",
      "dev: recall 0.638; precision 0.587; aer 0.390\n",
      "test: recall 0.650; precision 0.589; aer 0.384\n"
     ]
    }
   ],
   "source": [
    "_, translation_model1, _ = train(10, hard_align=True, use_null=True)\n",
    "_, translation_model2, _  = train(6, prior_model_cls=ImprovedComplexPriorModel,\n",
    "                                  translation_model=translation_model1,\n",
    "                                  num_indices=20, use_null=True)\n",
    "test_alignments, translation_model_hmm, _ = train(5, prior_model_cls=TransitionModel,\n",
    "                                                  translation_model=translation_model2,\n",
    "                                                  hmm=True, use_null=True)\n",
    "evaluate(test_alignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null tokens improved AER, but not by much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using lemmas and edidistance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we reduce number of different words in our corpora by mapping them to their lowercase lemmas. Also we improve hard-alignment by setting alignment probability to 1 when tokens have small editdistance compared to the source word length (for example $\\frac{edit\\_distance}{source\\_word\\_length} < 0.2$) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only using editdistance and nulls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus log likelihood: -1390590.154\n",
      "dev: recall 0.439; precision 0.396; aer 0.584\n",
      "test: recall 0.443; precision 0.395; aer 0.583\n",
      "corpus log likelihood: -1259842.567\n",
      "dev: recall 0.479; precision 0.431; aer 0.547\n",
      "test: recall 0.480; precision 0.427; aer 0.549\n",
      "corpus log likelihood: -1203119.442\n",
      "dev: recall 0.494; precision 0.444; aer 0.534\n",
      "test: recall 0.492; precision 0.438; aer 0.537\n",
      "corpus log likelihood: -1180431.644\n",
      "dev: recall 0.499; precision 0.449; aer 0.528\n",
      "test: recall 0.499; precision 0.443; aer 0.532\n",
      "corpus log likelihood: -1170298.505\n",
      "dev: recall 0.503; precision 0.452; aer 0.525\n",
      "test: recall 0.503; precision 0.447; aer 0.527\n",
      "corpus log likelihood: -1165042.450\n",
      "dev: recall 0.505; precision 0.453; aer 0.523\n",
      "test: recall 0.505; precision 0.449; aer 0.526\n",
      "corpus log likelihood: -1161982.313\n",
      "dev: recall 0.506; precision 0.454; aer 0.522\n",
      "test: recall 0.507; precision 0.450; aer 0.524\n",
      "corpus log likelihood: -1160046.289\n",
      "dev: recall 0.508; precision 0.456; aer 0.520\n",
      "test: recall 0.508; precision 0.451; aer 0.523\n",
      "corpus log likelihood: -1158744.537\n",
      "dev: recall 0.509; precision 0.457; aer 0.519\n",
      "test: recall 0.508; precision 0.451; aer 0.523\n",
      "corpus log likelihood: -1306988.319\n",
      "dev: recall 0.613; precision 0.557; aer 0.418\n",
      "test: recall 0.621; precision 0.557; aer 0.414\n",
      "corpus log likelihood: -1265217.590\n",
      "dev: recall 0.623; precision 0.567; aer 0.408\n",
      "test: recall 0.629; precision 0.565; aer 0.406\n",
      "corpus log likelihood: -1242956.326\n",
      "dev: recall 0.626; precision 0.570; aer 0.405\n",
      "test: recall 0.633; precision 0.569; aer 0.402\n",
      "corpus log likelihood: -1227391.988\n",
      "dev: recall 0.627; precision 0.572; aer 0.403\n",
      "test: recall 0.637; precision 0.573; aer 0.398\n",
      "corpus log likelihood: -1215938.523\n",
      "dev: recall 0.627; precision 0.573; aer 0.402\n",
      "test: recall 0.636; precision 0.572; aer 0.399\n",
      "corpus log likelihood: -978258.167\n",
      "dev: recall 0.639; precision 0.584; aer 0.391\n",
      "test: recall 0.649; precision 0.585; aer 0.386\n",
      "corpus log likelihood: -919349.264\n",
      "dev: recall 0.641; precision 0.587; aer 0.389\n",
      "test: recall 0.652; precision 0.589; aer 0.382\n",
      "corpus log likelihood: -890865.470\n",
      "dev: recall 0.642; precision 0.589; aer 0.387\n",
      "test: recall 0.653; precision 0.591; aer 0.381\n",
      "corpus log likelihood: -874295.886\n",
      "dev: recall 0.642; precision 0.590; aer 0.387\n",
      "test: recall 0.654; precision 0.593; aer 0.379\n",
      "dev: recall 0.642; precision 0.590; aer 0.387\n",
      "test: recall 0.654; precision 0.593; aer 0.379\n"
     ]
    }
   ],
   "source": [
    "_, translation_model1, _ = train(10, hard_align=True, use_null=True, use_editdistance=True)\n",
    "_, translation_model2, _  = train(6, prior_model_cls=ImprovedComplexPriorModel,\n",
    "                                  translation_model=translation_model1,\n",
    "                                  num_indices=20, use_null=True)\n",
    "test_alignments, translation_model_hmm, _ = train(5, prior_model_cls=TransitionModel,\n",
    "                                                  translation_model=translation_model2,\n",
    "                                                  hmm=True, use_null=True)\n",
    "evaluate(test_alignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding lemmas from [this repo](https://github.com/michmech/lemmatization-lists) (needs to be cloned and placed alongside this notebook). Using english lemmas for english and Czech, Slovak and Slovene lemmas for Czech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus log likelihood: -1362528.658\n",
      "dev: recall 0.549; precision 0.490; aer 0.483\n",
      "test: recall 0.551; precision 0.487; aer 0.484\n",
      "corpus log likelihood: -1206680.969\n",
      "dev: recall 0.577; precision 0.516; aer 0.457\n",
      "test: recall 0.578; precision 0.510; aer 0.459\n",
      "corpus log likelihood: -1144326.083\n",
      "dev: recall 0.586; precision 0.523; aer 0.448\n",
      "test: recall 0.586; precision 0.518; aer 0.451\n",
      "corpus log likelihood: -1122879.060\n",
      "dev: recall 0.591; precision 0.528; aer 0.443\n",
      "test: recall 0.591; precision 0.522; aer 0.447\n",
      "corpus log likelihood: -1113927.201\n",
      "dev: recall 0.595; precision 0.532; aer 0.439\n",
      "test: recall 0.593; precision 0.525; aer 0.444\n",
      "corpus log likelihood: -1109406.550\n",
      "dev: recall 0.597; precision 0.533; aer 0.438\n",
      "test: recall 0.596; precision 0.527; aer 0.442\n",
      "corpus log likelihood: -1106804.159\n",
      "dev: recall 0.598; precision 0.533; aer 0.437\n",
      "test: recall 0.597; precision 0.528; aer 0.441\n",
      "corpus log likelihood: -1105162.271\n",
      "dev: recall 0.598; precision 0.533; aer 0.437\n",
      "test: recall 0.598; precision 0.529; aer 0.440\n",
      "corpus log likelihood: -1104055.781\n",
      "dev: recall 0.599; precision 0.534; aer 0.436\n",
      "test: recall 0.598; precision 0.529; aer 0.440\n",
      "corpus log likelihood: -1234256.750\n",
      "dev: recall 0.695; precision 0.628; aer 0.341\n",
      "test: recall 0.701; precision 0.626; aer 0.340\n",
      "corpus log likelihood: -1197544.408\n",
      "dev: recall 0.699; precision 0.633; aer 0.337\n",
      "test: recall 0.706; precision 0.631; aer 0.335\n",
      "corpus log likelihood: -1183331.503\n",
      "dev: recall 0.701; precision 0.635; aer 0.335\n",
      "test: recall 0.710; precision 0.635; aer 0.331\n",
      "corpus log likelihood: -1174248.100\n",
      "dev: recall 0.702; precision 0.636; aer 0.334\n",
      "test: recall 0.712; precision 0.637; aer 0.329\n",
      "corpus log likelihood: -1167593.354\n",
      "dev: recall 0.701; precision 0.636; aer 0.335\n",
      "test: recall 0.713; precision 0.639; aer 0.327\n",
      "corpus log likelihood: -927718.557\n",
      "dev: recall 0.725; precision 0.657; aer 0.312\n",
      "test: recall 0.733; precision 0.656; aer 0.309\n",
      "corpus log likelihood: -868029.812\n",
      "dev: recall 0.729; precision 0.662; aer 0.307\n",
      "test: recall 0.738; precision 0.661; aer 0.304\n",
      "corpus log likelihood: -843394.511\n",
      "dev: recall 0.731; precision 0.664; aer 0.305\n",
      "test: recall 0.741; precision 0.666; aer 0.300\n",
      "corpus log likelihood: -829885.859\n",
      "dev: recall 0.733; precision 0.666; aer 0.304\n",
      "test: recall 0.742; precision 0.668; aer 0.298\n",
      "dev: recall 0.733; precision 0.666; aer 0.304\n",
      "test: recall 0.742; precision 0.668; aer 0.298\n"
     ]
    }
   ],
   "source": [
    "_, translation_model1, _ = train(10, hard_align=True, use_null=True, use_editdistance=True,\n",
    "                                 use_lemmas=True)\n",
    "_, translation_model2, _  = train(6, prior_model_cls=ImprovedComplexPriorModel,\n",
    "                                  translation_model=translation_model1,\n",
    "                                  num_indices=20, use_null=True, use_lemmas=True)\n",
    "test_alignments, translation_model_hmm, _ = train(5, prior_model_cls=TransitionModel,\n",
    "                                                  translation_model=translation_model2,\n",
    "                                                  hmm=True, use_null=True, use_lemmas=True)\n",
    "evaluate(test_alignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further decrease the number of parameters we simplify things by mapping words to indices using hash function: $word\\_idx = hash(word)\\ \\%\\ num\\_buckets$.\n",
    "The final model has num_buckets=3000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/penguin138/.virtualenvs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:41: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus log likelihood: -1433400.254\n",
      "dev: recall 0.540; precision 0.484; aer 0.490\n",
      "test: recall 0.548; precision 0.486; aer 0.486\n",
      "corpus log likelihood: -1310437.416\n",
      "dev: recall 0.557; precision 0.498; aer 0.475\n",
      "test: recall 0.563; precision 0.498; aer 0.472\n",
      "corpus log likelihood: -1248885.513\n",
      "dev: recall 0.564; precision 0.504; aer 0.468\n",
      "test: recall 0.570; precision 0.503; aer 0.466\n",
      "corpus log likelihood: -1225062.843\n",
      "dev: recall 0.570; precision 0.509; aer 0.463\n",
      "test: recall 0.573; precision 0.506; aer 0.463\n",
      "corpus log likelihood: -1214351.537\n",
      "dev: recall 0.574; precision 0.512; aer 0.460\n",
      "test: recall 0.577; precision 0.509; aer 0.460\n",
      "corpus log likelihood: -1208728.004\n",
      "dev: recall 0.578; precision 0.515; aer 0.456\n",
      "test: recall 0.578; precision 0.510; aer 0.459\n",
      "corpus log likelihood: -1205440.348\n",
      "dev: recall 0.579; precision 0.517; aer 0.455\n",
      "test: recall 0.579; precision 0.511; aer 0.458\n",
      "corpus log likelihood: -1203367.560\n",
      "dev: recall 0.580; precision 0.518; aer 0.454\n",
      "test: recall 0.580; precision 0.512; aer 0.457\n",
      "corpus log likelihood: -1201982.261\n",
      "dev: recall 0.581; precision 0.518; aer 0.453\n",
      "test: recall 0.582; precision 0.513; aer 0.456\n",
      "corpus log likelihood: -1332392.452\n",
      "dev: recall 0.680; precision 0.614; aer 0.356\n",
      "test: recall 0.688; precision 0.614; aer 0.352\n",
      "corpus log likelihood: -1298265.286\n",
      "dev: recall 0.687; precision 0.622; aer 0.349\n",
      "test: recall 0.698; precision 0.624; aer 0.343\n",
      "corpus log likelihood: -1284559.706\n",
      "dev: recall 0.692; precision 0.628; aer 0.343\n",
      "test: recall 0.703; precision 0.630; aer 0.337\n",
      "corpus log likelihood: -1274976.165\n",
      "dev: recall 0.697; precision 0.633; aer 0.338\n",
      "test: recall 0.706; precision 0.633; aer 0.334\n",
      "corpus log likelihood: -1267667.931\n",
      "dev: recall 0.700; precision 0.636; aer 0.335\n",
      "test: recall 0.708; precision 0.636; aer 0.331\n",
      "corpus log likelihood: -1037762.287\n",
      "dev: recall 0.716; precision 0.651; aer 0.319\n",
      "test: recall 0.726; precision 0.651; aer 0.315\n",
      "corpus log likelihood: -975491.926\n",
      "dev: recall 0.725; precision 0.661; aer 0.310\n",
      "test: recall 0.735; precision 0.661; aer 0.305\n",
      "corpus log likelihood: -948573.668\n",
      "dev: recall 0.729; precision 0.666; aer 0.305\n",
      "test: recall 0.739; precision 0.667; aer 0.300\n",
      "corpus log likelihood: -933496.146\n",
      "dev: recall 0.732; precision 0.670; aer 0.302\n",
      "test: recall 0.744; precision 0.673; aer 0.295\n"
     ]
    }
   ],
   "source": [
    "_, translation_model1, _ = train(10, hard_align=True, use_null=True, use_editdistance=True,\n",
    "                                 use_lemmas=True, use_hashing=True)\n",
    "_, translation_model2, _  = train(6, prior_model_cls=ImprovedComplexPriorModel,\n",
    "                                  translation_model=translation_model1,\n",
    "                                  num_indices=20, use_null=True, use_lemmas=True, use_hashing=True)\n",
    "test_alignments, translation_model_hmm, _ = train(5, prior_model_cls=TransitionModel,\n",
    "                                                  translation_model=translation_model2,\n",
    "                                                  hmm=True, use_null=True, use_lemmas=True,\n",
    "                                                  use_hashing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hashing didn't help  to decrease AER, now let's try to do several runs of HMM using fresh TransitionModels, because Translation Model will keep improving and Transition model won't diverge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/penguin138/.virtualenvs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:41: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus log likelihood: -1433400.254\n",
      "dev: recall 0.540; precision 0.484; aer 0.490\n",
      "test: recall 0.548; precision 0.486; aer 0.486\n",
      "corpus log likelihood: -1310437.416\n",
      "dev: recall 0.557; precision 0.498; aer 0.475\n",
      "test: recall 0.563; precision 0.498; aer 0.472\n",
      "corpus log likelihood: -1248885.513\n",
      "dev: recall 0.564; precision 0.504; aer 0.468\n",
      "test: recall 0.570; precision 0.503; aer 0.466\n",
      "corpus log likelihood: -1225062.843\n",
      "dev: recall 0.570; precision 0.509; aer 0.463\n",
      "test: recall 0.573; precision 0.506; aer 0.463\n",
      "corpus log likelihood: -1214351.537\n",
      "dev: recall 0.574; precision 0.512; aer 0.460\n",
      "test: recall 0.577; precision 0.509; aer 0.460\n",
      "corpus log likelihood: -1208728.004\n",
      "dev: recall 0.578; precision 0.515; aer 0.456\n",
      "test: recall 0.578; precision 0.510; aer 0.459\n",
      "corpus log likelihood: -1205440.348\n",
      "dev: recall 0.579; precision 0.517; aer 0.455\n",
      "test: recall 0.579; precision 0.511; aer 0.458\n",
      "corpus log likelihood: -1203367.560\n",
      "dev: recall 0.580; precision 0.518; aer 0.454\n",
      "test: recall 0.580; precision 0.512; aer 0.457\n",
      "corpus log likelihood: -1201982.261\n",
      "dev: recall 0.581; precision 0.518; aer 0.453\n",
      "test: recall 0.582; precision 0.513; aer 0.456\n",
      "corpus log likelihood: -1332392.452\n",
      "dev: recall 0.680; precision 0.614; aer 0.356\n",
      "test: recall 0.688; precision 0.614; aer 0.352\n",
      "corpus log likelihood: -1298265.286\n",
      "dev: recall 0.687; precision 0.622; aer 0.349\n",
      "test: recall 0.698; precision 0.624; aer 0.343\n",
      "corpus log likelihood: -1284559.706\n",
      "dev: recall 0.692; precision 0.628; aer 0.343\n",
      "test: recall 0.703; precision 0.630; aer 0.337\n",
      "corpus log likelihood: -1274976.165\n",
      "dev: recall 0.697; precision 0.633; aer 0.338\n",
      "test: recall 0.706; precision 0.633; aer 0.334\n",
      "corpus log likelihood: -1267667.931\n",
      "dev: recall 0.700; precision 0.636; aer 0.335\n",
      "test: recall 0.708; precision 0.636; aer 0.331\n",
      "corpus log likelihood: -1037762.287\n",
      "dev: recall 0.716; precision 0.651; aer 0.319\n",
      "test: recall 0.726; precision 0.651; aer 0.315\n",
      "corpus log likelihood: -975491.926\n",
      "dev: recall 0.725; precision 0.661; aer 0.310\n",
      "test: recall 0.735; precision 0.661; aer 0.305\n",
      "corpus log likelihood: -948573.668\n",
      "dev: recall 0.729; precision 0.666; aer 0.305\n",
      "test: recall 0.739; precision 0.667; aer 0.300\n",
      "corpus log likelihood: -933496.146\n",
      "dev: recall 0.732; precision 0.670; aer 0.302\n",
      "test: recall 0.744; precision 0.673; aer 0.295\n",
      "corpus log likelihood: -1011511.537\n",
      "dev: recall 0.731; precision 0.667; aer 0.304\n",
      "test: recall 0.743; precision 0.670; aer 0.297\n",
      "corpus log likelihood: -946540.903\n",
      "dev: recall 0.734; precision 0.671; aer 0.300\n",
      "test: recall 0.744; precision 0.673; aer 0.295\n",
      "corpus log likelihood: -925539.668\n",
      "dev: recall 0.735; precision 0.672; aer 0.299\n",
      "test: recall 0.746; precision 0.675; aer 0.293\n",
      "corpus log likelihood: -916096.421\n",
      "dev: recall 0.735; precision 0.673; aer 0.299\n",
      "test: recall 0.746; precision 0.676; aer 0.293\n",
      "corpus log likelihood: -1004446.500\n",
      "dev: recall 0.735; precision 0.672; aer 0.300\n",
      "test: recall 0.747; precision 0.675; aer 0.292\n",
      "corpus log likelihood: -935886.228\n",
      "dev: recall 0.736; precision 0.674; aer 0.298\n",
      "test: recall 0.747; precision 0.676; aer 0.292\n",
      "corpus log likelihood: -916073.461\n",
      "dev: recall 0.736; precision 0.674; aer 0.298\n",
      "test: recall 0.747; precision 0.677; aer 0.291\n",
      "corpus log likelihood: -908433.977\n",
      "dev: recall 0.736; precision 0.675; aer 0.297\n",
      "test: recall 0.748; precision 0.678; aer 0.290\n",
      "dev: recall 0.736; precision 0.675; aer 0.297\n",
      "test: recall 0.748; precision 0.678; aer 0.290\n"
     ]
    }
   ],
   "source": [
    "_, translation_model1, _ = train(10, hard_align=True, use_null=True, use_editdistance=True,\n",
    "                                 use_lemmas=True, use_hashing=True)\n",
    "_, translation_model2, _  = train(6, prior_model_cls=ImprovedComplexPriorModel,\n",
    "                                  translation_model=translation_model1,\n",
    "                                  num_indices=20, use_null=True, use_lemmas=True, use_hashing=True)\n",
    "test_alignments, translation_model_hmm, _ = train(5, prior_model_cls=TransitionModel,\n",
    "                                                  translation_model=translation_model2,\n",
    "                                                  hmm=True, use_null=True, use_lemmas=True,\n",
    "                                                  use_hashing=True)\n",
    "\n",
    "test_alignments, translation_model_hmm, _ = train(5, prior_model_cls=TransitionModel,\n",
    "                                                  translation_model=translation_model_hmm,\n",
    "                                                  hmm=True, use_null=True, use_lemmas=True,\n",
    "                                                  use_hashing=True)\n",
    "\n",
    "test_alignments, translation_model_hmm, _ = train(5, prior_model_cls=TransitionModel,\n",
    "                                                  translation_model=translation_model_hmm,\n",
    "                                                  hmm=True, use_null=True, use_lemmas=True,\n",
    "                                                  use_hashing=True)\n",
    "\n",
    "evaluate(test_alignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how far we can push this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/penguin138/.virtualenvs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:41: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus log likelihood: -1433400.254\n",
      "dev: recall 0.540; precision 0.484; aer 0.490\n",
      "test: recall 0.548; precision 0.486; aer 0.486\n",
      "corpus log likelihood: -1310437.416\n",
      "dev: recall 0.557; precision 0.498; aer 0.475\n",
      "test: recall 0.563; precision 0.498; aer 0.472\n",
      "corpus log likelihood: -1248885.513\n",
      "dev: recall 0.564; precision 0.504; aer 0.468\n",
      "test: recall 0.570; precision 0.503; aer 0.466\n",
      "corpus log likelihood: -1225062.843\n",
      "dev: recall 0.570; precision 0.509; aer 0.463\n",
      "test: recall 0.573; precision 0.506; aer 0.463\n",
      "corpus log likelihood: -1214351.537\n",
      "dev: recall 0.574; precision 0.512; aer 0.460\n",
      "test: recall 0.577; precision 0.509; aer 0.460\n",
      "corpus log likelihood: -1208728.004\n",
      "dev: recall 0.578; precision 0.515; aer 0.456\n",
      "test: recall 0.578; precision 0.510; aer 0.459\n",
      "corpus log likelihood: -1205440.348\n",
      "dev: recall 0.579; precision 0.517; aer 0.455\n",
      "test: recall 0.579; precision 0.511; aer 0.458\n",
      "corpus log likelihood: -1203367.560\n",
      "dev: recall 0.580; precision 0.518; aer 0.454\n",
      "test: recall 0.580; precision 0.512; aer 0.457\n",
      "corpus log likelihood: -1201982.261\n",
      "dev: recall 0.581; precision 0.518; aer 0.453\n",
      "test: recall 0.582; precision 0.513; aer 0.456\n",
      "corpus log likelihood: -1332392.452\n",
      "dev: recall 0.680; precision 0.614; aer 0.356\n",
      "test: recall 0.688; precision 0.614; aer 0.352\n",
      "corpus log likelihood: -1298265.286\n",
      "dev: recall 0.687; precision 0.622; aer 0.349\n",
      "test: recall 0.698; precision 0.624; aer 0.343\n",
      "corpus log likelihood: -1284559.706\n",
      "dev: recall 0.692; precision 0.628; aer 0.343\n",
      "test: recall 0.703; precision 0.630; aer 0.337\n",
      "corpus log likelihood: -1274976.165\n",
      "dev: recall 0.697; precision 0.633; aer 0.338\n",
      "test: recall 0.706; precision 0.633; aer 0.334\n",
      "corpus log likelihood: -1267667.931\n",
      "dev: recall 0.700; precision 0.636; aer 0.335\n",
      "test: recall 0.708; precision 0.636; aer 0.331\n",
      "corpus log likelihood: -1037762.287\n",
      "dev: recall 0.716; precision 0.651; aer 0.319\n",
      "test: recall 0.726; precision 0.651; aer 0.315\n",
      "corpus log likelihood: -975491.926\n",
      "dev: recall 0.725; precision 0.661; aer 0.310\n",
      "test: recall 0.735; precision 0.661; aer 0.305\n",
      "corpus log likelihood: -948573.668\n",
      "dev: recall 0.729; precision 0.666; aer 0.305\n",
      "test: recall 0.739; precision 0.667; aer 0.300\n",
      "corpus log likelihood: -933496.146\n",
      "dev: recall 0.732; precision 0.670; aer 0.302\n",
      "test: recall 0.744; precision 0.673; aer 0.295\n",
      "corpus log likelihood: -1011511.537\n",
      "dev: recall 0.731; precision 0.667; aer 0.304\n",
      "test: recall 0.743; precision 0.670; aer 0.297\n",
      "corpus log likelihood: -946540.903\n",
      "dev: recall 0.734; precision 0.671; aer 0.300\n",
      "test: recall 0.744; precision 0.673; aer 0.295\n",
      "corpus log likelihood: -925539.668\n",
      "dev: recall 0.735; precision 0.672; aer 0.299\n",
      "test: recall 0.746; precision 0.675; aer 0.293\n",
      "corpus log likelihood: -916096.421\n",
      "dev: recall 0.735; precision 0.673; aer 0.299\n",
      "test: recall 0.746; precision 0.676; aer 0.293\n",
      "corpus log likelihood: -1004446.500\n",
      "dev: recall 0.735; precision 0.672; aer 0.300\n",
      "test: recall 0.747; precision 0.675; aer 0.292\n",
      "corpus log likelihood: -935886.228\n",
      "dev: recall 0.736; precision 0.674; aer 0.298\n",
      "test: recall 0.747; precision 0.676; aer 0.292\n",
      "corpus log likelihood: -916073.461\n",
      "dev: recall 0.736; precision 0.674; aer 0.298\n",
      "test: recall 0.747; precision 0.677; aer 0.291\n",
      "corpus log likelihood: -908433.977\n",
      "dev: recall 0.736; precision 0.675; aer 0.297\n",
      "test: recall 0.748; precision 0.678; aer 0.290\n",
      "corpus log likelihood: -1001487.575\n",
      "dev: recall 0.738; precision 0.675; aer 0.297\n",
      "test: recall 0.748; precision 0.676; aer 0.291\n",
      "corpus log likelihood: -930721.520\n",
      "dev: recall 0.737; precision 0.675; aer 0.297\n",
      "test: recall 0.748; precision 0.678; aer 0.290\n",
      "corpus log likelihood: -911179.750\n",
      "dev: recall 0.738; precision 0.676; aer 0.296\n",
      "test: recall 0.749; precision 0.680; aer 0.289\n",
      "corpus log likelihood: -904284.000\n",
      "dev: recall 0.738; precision 0.677; aer 0.295\n",
      "test: recall 0.750; precision 0.681; aer 0.288\n",
      "corpus log likelihood: -999960.301\n",
      "dev: recall 0.739; precision 0.676; aer 0.295\n",
      "test: recall 0.749; precision 0.678; aer 0.290\n",
      "corpus log likelihood: -927793.346\n",
      "dev: recall 0.738; precision 0.677; aer 0.295\n",
      "test: recall 0.749; precision 0.680; aer 0.289\n",
      "corpus log likelihood: -908148.730\n",
      "dev: recall 0.739; precision 0.678; aer 0.294\n",
      "test: recall 0.749; precision 0.680; aer 0.288\n",
      "corpus log likelihood: -901596.147\n",
      "dev: recall 0.739; precision 0.678; aer 0.294\n",
      "test: recall 0.749; precision 0.681; aer 0.288\n",
      "corpus log likelihood: -999080.317\n",
      "dev: recall 0.740; precision 0.677; aer 0.294\n",
      "test: recall 0.749; precision 0.679; aer 0.289\n",
      "corpus log likelihood: -925881.168\n",
      "dev: recall 0.739; precision 0.677; aer 0.295\n",
      "test: recall 0.749; precision 0.680; aer 0.289\n",
      "corpus log likelihood: -906139.754\n",
      "dev: recall 0.738; precision 0.677; aer 0.295\n",
      "test: recall 0.749; precision 0.680; aer 0.289\n",
      "corpus log likelihood: -899740.971\n",
      "dev: recall 0.738; precision 0.678; aer 0.295\n",
      "test: recall 0.750; precision 0.681; aer 0.288\n",
      "dev: recall 0.738; precision 0.678; aer 0.295\n",
      "test: recall 0.750; precision 0.681; aer 0.288\n"
     ]
    }
   ],
   "source": [
    "_, translation_model1, _ = train(10, hard_align=True, use_null=True, use_editdistance=True,\n",
    "                                 use_lemmas=True, use_hashing=True)\n",
    "_, translation_model2, _  = train(6, prior_model_cls=ImprovedComplexPriorModel,\n",
    "                                  translation_model=translation_model1,\n",
    "                                  num_indices=20, use_null=True, use_lemmas=True, use_hashing=True)\n",
    "\n",
    "test_alignments, translation_model_hmm, _ = train(5, prior_model_cls=TransitionModel,\n",
    "                                                  translation_model=translation_model2,\n",
    "                                                  hmm=True, use_null=True, use_lemmas=True,\n",
    "                                                  use_hashing=True)\n",
    "num_repetitions = 5\n",
    "for i in range(num_repetitions):\n",
    "    test_alignments, translation_model_hmm, _ = train(5, prior_model_cls=TransitionModel,\n",
    "                                                      translation_model=translation_model_hmm,\n",
    "                                                      hmm=True, use_null=True, use_lemmas=True,\n",
    "                                                      use_hashing=True)\n",
    "\n",
    "evaluate(test_alignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, repeating HMM over and over didn't improve the AER significantly, so I guess the best model is the previous one (with only 3 repetitions of HMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discrete HMM with scaling. You may want to use this if you decide to implement an HMM.\n",
    "# The parameters for this HMM will still need to be provided by the models above.\n",
    "\n",
    "def forward(pi, A, O):\n",
    "    S, T = O.shape\n",
    "    alpha = np.zeros((S, T))\n",
    "    scaling_factors = np.zeros(T)\n",
    "    \n",
    "    # base case\n",
    "    alpha[:, 0] = pi * O[:, 0]\n",
    "    scaling_factors[0] = np.sum(alpha[:, 0])\n",
    "    alpha[:, 0] /= scaling_factors[0] \n",
    "    \n",
    "    # recursive case\n",
    "    for t in range(1, T):\n",
    "        alpha[:, t] = np.dot(alpha[:, t-1], A[:, :]) * O[:, t]\n",
    "\n",
    "        # Normalize at each step to prevent underflow.\n",
    "        scaling_factors[t] = np.sum(alpha[:, t])\n",
    "        alpha[:, t] /= scaling_factors[t]\n",
    "\n",
    "    return (alpha, scaling_factors)\n",
    "\n",
    "def backward(pi, A, O, forward_scaling_factors):\n",
    "    S, T = O.shape\n",
    "    beta = np.zeros((S, T))\n",
    "\n",
    "    # base case\n",
    "    beta[:, T-1] = 1 / forward_scaling_factors[T-1]\n",
    "    \n",
    "    # recursive case\n",
    "    for t in range(T-2, -1, -1):\n",
    "        beta[:, t] = np.sum(beta[:, t+1] * A[:, :] * O[:, t+1], 1) / forward_scaling_factors[t]\n",
    "\n",
    "    return beta\n",
    "\n",
    "def forward_backward(pi, A, O):\n",
    "    alpha, forward_scaling_factors = forward(pi, A, O)\n",
    "    beta = backward(pi, A, O, forward_scaling_factors)\n",
    "    return alpha, beta, np.sum(np.log(forward_scaling_factors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
